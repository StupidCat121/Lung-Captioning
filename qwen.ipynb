{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fd86991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_caption_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, requests, math\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87148a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 0) ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå\n",
    "# ----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9fdba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + ‡πÇ‡∏õ‡∏£‡πÄ‡∏ã‡∏™‡πÄ‡∏ã‡∏≠‡∏£‡πå\n",
    "# ----------------------------\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    dtype=\"auto\",         # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ torch>=2.4 ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ dtype=torch.float16/bfloat16\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    offload_folder=\"offload\",   # ‡∏ö‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ disk\n",
    "    quantization_config=bnb_cfg,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93112dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡∏ó‡∏î‡∏™‡∏≠‡∏ö + ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\n",
    "# ----------------------------\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "image = Image.open(BytesIO(requests.get(url, timeout=15).content)).convert(\"RGB\").resize((224, 224))\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "        {\"type\": \"image\", \"image\": image},\n",
    "        {\"type\": \"text\", \"text\": \"Describe this image.\"}\n",
    "    ],\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b64408d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: The image depicts a serene beach scene during what appears to be either sunrise or sunset\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3) Inference ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏£‡∏¥‡∏á\n",
    "# ----------------------------\n",
    "chat_text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á batch inputs (‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°+‡∏†‡∏≤‡∏û) ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö generate\n",
    "inputs = processor(\n",
    "    text=[chat_text],\n",
    "    images=[image],\n",
    "    videos=None,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gen_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=16,\n",
    "        do_sample=False,        # deterministic\n",
    "        use_cache=True,\n",
    "    )\n",
    "\n",
    "# ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å ‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏• \"‡∏û‡∏π‡∏î‡∏ï‡πà‡∏≠\"\n",
    "trimmed = [out[len(inp):] for inp, out in zip(inputs[\"input_ids\"], gen_ids)]\n",
    "texts = processor.batch_decode(trimmed, skip_special_tokens=True)\n",
    "print(\"Model output:\", texts[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51044de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vision_config', 'text_config', 'image_token_id', 'video_token_id', 'return_dict', 'output_hidden_states', 'torchscript', 'dtype', 'pruned_heads', 'tie_word_embeddings', 'chunk_size_feed_forward', 'is_encoder_decoder', 'is_decoder', 'cross_attention_hidden_size', 'add_cross_attention', 'tie_encoder_decoder', 'architectures', 'finetuning_task', 'id2label', 'label2id', 'task_specific_params', 'problem_type', 'tokenizer_class', 'prefix', 'bos_token_id', 'pad_token_id', 'eos_token_id', 'sep_token_id', 'decoder_start_token_id', 'max_length', 'min_length', 'do_sample', 'early_stopping', 'num_beams', 'temperature', 'top_k', 'top_p', 'typical_p', 'repetition_penalty', 'length_penalty', 'no_repeat_ngram_size', 'encoder_no_repeat_ngram_size', 'bad_words_ids', 'num_return_sequences', 'output_scores', 'return_dict_in_generate', 'forced_bos_token_id', 'forced_eos_token_id', 'remove_invalid_values', 'exponential_decay_length_penalty', 'suppress_tokens', 'begin_suppress_tokens', 'num_beam_groups', 'diversity_penalty', '_name_or_path', 'transformers_version', 'attention_dropout', 'vision_start_token_id', 'vision_end_token_id', 'vision_token_id', 'hidden_act', 'hidden_size', 'initializer_range', 'intermediate_size', 'max_position_embeddings', 'max_window_layers', 'model_type', 'num_attention_heads', 'num_hidden_layers', 'num_key_value_heads', 'rms_norm_eps', 'rope_theta', 'sliding_window', 'use_cache', 'use_sliding_window', 'rope_scaling', 'vocab_size', 'quantization_config', 'output_attentions'])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 4) ‡∏î‡∏∂‡∏á Visual Attention ‡∏à‡∏≤‡∏Å Vision Tower (ViT)\n",
    "#    - ‡πÉ‡∏ä‡πâ‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏à‡∏≤‡∏Å image_processor ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á\n",
    "#    - ‡∏Ç‡∏≠ output_attentions=True ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà attention ‡∏ó‡∏∏‡∏Å‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå\n",
    "# ----------------------------\n",
    "\n",
    "print(model.config.to_dict().keys())\n",
    "print(model.config.output_attentions)   # ‡∏Ñ‡πà‡∏≤ default (True/False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5e8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    out = model(\n",
    "        **inputs,\n",
    "        output_attentions=True,\n",
    "        return_dict=True,\n",
    "        use_cache=False\n",
    "    )\n",
    "\n",
    "print(type(out.attentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fd177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "(None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None)\n"
     ]
    }
   ],
   "source": [
    "print(len(out.attentions))      # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå\n",
    "print(out.attentions)           # ‡πÄ‡∏•‡πÄ‡∏¢‡∏≠‡∏£‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2e357",
   "metadata": {},
   "source": [
    "\n",
    "üîé ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏£‡∏∏‡πà‡∏ô Instruct\n",
    "\n",
    "* **Qwen2.5-VL-3B-Instruct** ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∏‡πà‡∏ô‡∏ó‡∏µ‡πà fine-tune ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ (chat) ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\n",
    "* ‡∏ó‡∏µ‡∏°‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏õ‡∏¥‡∏î `output_attentions` ‚Üí ‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠ attention ‡∏à‡∏∞‡πÑ‡∏î‡πâ `None`\n",
    "* hook ‡∏Å‡πá‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏Ñ‡πà‡∏≤ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ attention weights ‡∏ñ‡∏π‡∏Å‡∏ó‡∏¥‡πâ‡∏á‡πÑ‡∏õ‡πÉ‡∏ô forward\n",
    "\n",
    "‚úÖ ‡∏ß‡∏¥‡∏ò‡∏µ‡πÅ‡∏Å‡πâ\n",
    "* ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏õ‡πÉ‡∏ä‡πâ base ‡πÅ‡∏ó‡∏ô instruct \"Qwen/Qwen2.5-VL-3B\"\n",
    "* ‡πÅ‡∏ï‡πà‡∏ú‡∏π‡πâ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡∏î‡∏±‡∏ô‡∏•‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• base ‡πÑ‡∏õ‡∏à‡∏≤‡∏Å huggingface ‡πÅ‡∏•‡πâ‡∏ß ‚Üí ‡∏û‡∏≠‡πÑ‡∏õ‡∏ï‡∏≤‡∏°‡∏î‡∏π‡πÉ‡∏ô github ‡∏ñ‡∏∂‡∏á‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡∏ä‡∏∑‡πà‡∏≠ Qwen2.5-VL ‡πÅ‡∏ï‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô Instruct ‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß T_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e869ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # 5) ‡∏£‡∏µ‡πÄ‡∏ä‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏¥‡∏î‡πÅ‡∏û‡∏ï‡∏ä‡πå ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏±‡∏û‡∏™‡πÄ‡∏Å‡∏•‡∏ó‡∏±‡∏ö‡∏†‡∏≤‡∏û\n",
    "# #    - ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡∏£‡∏¥‡∏î‡∏à‡∏≤‡∏Å image_processor\n",
    "# #    - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ViT base (patch=14x14 ‡∏ó‡∏µ‡πà 224), ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏û‡∏¥‡∏Å‡πÄ‡∏ã‡∏•‡∏à‡∏£‡∏¥‡∏á‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏Å‡∏ß‡πà‡∏≤\n",
    "# # ----------------------------\n",
    "# # ‡∏´‡∏≤ H,W ‡∏´‡∏•‡∏±‡∏á preprocess ‡πÅ‡∏•‡πâ‡∏ß‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÅ‡∏û‡∏ï‡∏ä‡πå‡πÇ‡∏î‡∏¢‡∏î‡∏π‡∏à‡∏≤‡∏Å config ‡∏Ç‡∏≠‡∏á vision_model\n",
    "# # ‡∏ñ‡πâ‡∏≤ vision_model.config.patch_size ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ\n",
    "# if hasattr(vision_model.config, \"image_size\"):\n",
    "#     img_size_proc = vision_model.config.image_size  # ‡∏õ‡∏Å‡∏ï‡∏¥ 224 ‡∏´‡∏£‡∏∑‡∏≠ 448 (‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö processor)\n",
    "# else:\n",
    "#     # ‡∏™‡∏≥‡∏£‡∏≠‡∏á: ‡πÉ‡∏ä‡πâ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á pixel_values\n",
    "#     _, _, Hpx, Wpx = vision_inputs[\"pixel_values\"].shape\n",
    "#     img_size_proc = max(Hpx, Wpx)\n",
    "\n",
    "# patch = getattr(vision_model.config, \"patch_size\", 14)  # ‡∏™‡∏≥‡∏£‡∏≠‡∏á‡∏Ñ‡πà‡∏≤ 14 ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏£‡∏∞‡∏ö‡∏∏\n",
    "# Hp = math.ceil(img_size_proc / patch)\n",
    "# Wp = math.ceil(img_size_proc / patch)\n",
    "\n",
    "# # ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÇ‡∏ó‡πÄ‡∏Ñ‡πá‡∏ô‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏£‡∏¥‡∏î‡πÑ‡∏´‡∏° ‡πÑ‡∏°‡πà‡∏ï‡∏£‡∏á‡∏Å‡πá‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ö‡∏ö‡∏¢‡∏∑‡∏î‡∏´‡∏¢‡∏∏‡πà‡∏ô\n",
    "# if attn_mean.numel() != Hp * Wp:\n",
    "#     # ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏°‡∏µ token ‡πÑ‡∏°‡πà‡∏Ñ‡∏£‡∏ö‡πÄ‡∏ï‡πá‡∏°‡∏Å‡∏£‡∏¥‡∏î‡πÄ‡∏û‡∏£‡∏≤‡∏∞ padding; ‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤‡∏ß‡πà‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏∑‡∏≠ (N-1)\n",
    "#     # ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ sqrt ‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Å‡∏£‡∏¥‡∏î‡∏™‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏µ‡πà‡∏¢‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n",
    "#     Np = attn_mean.numel()\n",
    "#     s = int(round(math.sqrt(Np)))\n",
    "#     Hp = s\n",
    "#     Wp = Np // s\n",
    "\n",
    "# fmap = attn_mean.reshape(Hp, Wp).numpy()\n",
    "# fmap = (fmap - fmap.min()) / (fmap.max() - fmap.min() + 1e-6)\n",
    "\n",
    "# # ‡∏≠‡∏±‡∏û‡∏™‡πÄ‡∏Å‡∏• heatmap ‡πÉ‡∏´‡πâ‡πÄ‡∏ó‡πà‡∏≤‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö\n",
    "# img_w, img_h = image.size\n",
    "# fmap_img = np.array(Image.fromarray((fmap * 255).astype(np.uint8)).resize((img_w, img_h), Image.BILINEAR)) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f47e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # 6) ‡∏ß‡∏≤‡∏î heatmap ‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö + ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏•‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
    "# # ----------------------------\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.imshow(image)\n",
    "# plt.imshow(fmap_img, alpha=0.5, cmap=\"jet\")  # ‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö\n",
    "# plt.axis(\"off\")\n",
    "# plt.tight_layout()\n",
    "# out_path = \"attention_overlay.png\"\n",
    "# plt.savefig(out_path, dpi=150)\n",
    "# plt.close()\n",
    "# print(f\"Saved visual attention overlay to: {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung_caption_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
