{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51504a3",
   "metadata": {},
   "source": [
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ PyTorch ‡πÄ‡∏´‡πá‡∏ô GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "492cb47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.18\n",
      "Torch: 2.8.0+cu129\n",
      "Built with CUDA: 12.9\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Built with CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089eb27b",
   "metadata": {},
   "source": [
    "### 1. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Vision-Language\n",
    "\n",
    "- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Qwen2.5-VL (Vision-Language model) ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û + ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "- tokenizer ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (tokens)\n",
    "- load_in_4bit=True ‚Üí ‡πÉ‡∏ä‡πâ quantization 4-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad82eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 00:56:59.847000 12740 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd612244",
   "metadata": {},
   "source": [
    "### 2. ‡πÄ‡∏û‡∏¥‡πà‡∏° LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ LoRA (Low-Rank Adaptation) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ train ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "- ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ù‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ vision part ‡∏´‡∏£‡∏∑‡∏≠ language part ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7361ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84ae093",
   "metadata": {},
   "source": [
    "### 3. ‡πÇ‡∏´‡∏•‡∏î Dataset\n",
    "\n",
    "- ‡∏Å‡∏±‡∏ô train&eval 85% ‡πÑ‡∏ß‡πâ‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏≥ 5-Fold ‡∏ö‡∏ô‡∏ä‡∏∏‡∏î‡∏ù‡∏∂‡∏Å ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏°‡∏µ 15% ‡πÑ‡∏ß‡πâ‡πÄ‡∏õ‡πá‡∏ô blind test ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "- ‡πÉ‡∏ä‡πâ 5-Fold ‡∏ö‡∏ô train_hf (85%) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏π‡∏ô/‡πÄ‡∏•‡∏∑‡∏≠‡∏Å hyperparams ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°\n",
    "- ‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‚Üí ‡πÄ‡∏ó‡∏£‡∏ô‡πÉ‡∏´‡∏°‡πà‡∏ö‡∏ô train_hf ‚à™ val_hf (85%) ‡πÅ‡∏•‡πâ‡∏ß‡∏ß‡∏±‡∏î‡∏ö‡∏ô test_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ddbfb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 6085\n",
      "})\n",
      "{'image': Image(mode=None, decode=True), 'text': Value('string'), '__class__': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "print(hf)\n",
    "print(hf.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efd1da6",
   "metadata": {},
   "source": [
    "‡πÅ‡∏ö‡πà‡∏á dataset hf ‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 85% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö train ‡πÅ‡∏•‡∏∞ 15% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö blind test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80fbbd15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n"
     ]
    }
   ],
   "source": [
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f74aa9",
   "metadata": {},
   "source": [
    "‡∏î‡∏±‡∏á‡∏ô‡∏±‡πâ‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô K ‡∏ó‡∏µ‡πà ‚Äú‡∏•‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡πä‡∏∞‚Äù (‡∏Ç‡∏ô‡∏≤‡∏î‡πÅ‡∏ï‡πà‡∏•‡∏∞ fold ‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡∏û‡∏≠‡∏î‡∏µ) ‡∏Ñ‡∏∑‡∏≠:\n",
    "\n",
    "K = 2 ‚Üí 2,586/fold\n",
    "\n",
    "K = 3 ‚Üí 1,724/fold\n",
    "\n",
    "K = 4 ‚Üí 1,293/fold\n",
    "\n",
    "K = 6 ‚Üí 862/fold ‚Üê ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡πÉ‡∏Å‡∏•‡πâ‡∏Å‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô, granularity ‡∏î‡∏µ, ‡∏†‡∏≤‡∏£‡∏∞‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡∏ï‡πå‡∏û‡∏≠‡πÄ‡∏´‡∏°‡∏≤‡∏∞)\n",
    "\n",
    "‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á train_hf ‡πÄ‡∏õ‡πá‡∏ô 6 ‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô ‡πÅ‡∏•‡πâ‡∏ß ‡∏£‡∏ß‡∏° 5 ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏£‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Å‡∏±‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "=> 1 ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏à‡∏∞‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô val data ‡∏ï‡πà‡∏≠‡πÑ‡∏õ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d46fd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "Selected folds : [0, 1, 2, 3, 4]\n",
      "Combined size  : 4310\n"
     ]
    }
   ],
   "source": [
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å fold ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏° (‡πÄ‡∏ä‡πà‡∏ô 0-4)\n",
    "# selected_folds = [1, 2, 3, 4, 5]        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# selected_folds = [0, 2, 3, 4, 5]        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# selected_folds = [0, 1, 3, 4, 5]        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# selected_folds = [0, 1, 2, 4, 5]        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# selected_folds = [0, 1, 2, 3, 5]        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "selected_folds = [0, 1, 2, 3, 4]        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô list ‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "train_selected = concatenate_datasets([folds[i] for i in selected_folds])\n",
    "# idx_selected = 0                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# idx_selected = 1                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# idx_selected = 2                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# idx_selected = 3                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "# idx_selected = 4                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "idx_selected = 5                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(f\"Selected folds : {selected_folds}\")\n",
    "print(f\"Combined size  : {len(train_selected)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3991da5",
   "metadata": {},
   "source": [
    "### 4. ‡πÅ‡∏õ‡∏•‡∏á Dataset ‡πÄ‡∏õ‡πá‡∏ô Conversation Format\n",
    "\n",
    "- ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Chat (user ‚Üí assistant)\n",
    "- user ‚Üí ‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û + ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\n",
    "- assistant ‚Üí ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc449b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.'},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.JpegImagePlugin.JpegImageFile image mode=L size=450x450>}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Class: Normal\\nExplanation: Normal'}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    cls_name = sample[\"__class__\"]\n",
    "    description = sample[\"text\"]\n",
    "\n",
    "    answer = f\"Class: {cls_name}\\nExplanation: {description}\"\n",
    "    \n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        {\"role\" : \"assistant\", \"content\" : [\n",
    "            {\"type\" : \"text\", \"text\" : answer} ]\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\" : conversation}\n",
    "\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in train_selected]\n",
    "\n",
    "converted_dataset_val = [convert_to_conversation(sample) for sample in val_selected]\n",
    "\n",
    "converted_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331cc3e5",
   "metadata": {},
   "source": [
    "### 5. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Training)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ SFTTrainer ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ supervised fine-tuning\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• converted_dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà (‡∏†‡∏≤‡∏û ‚Üí ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
    "- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î hyperparameters ‡πÄ‡∏ä‡πà‡∏ô batch size, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ed5d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î metric BLEU ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï\n",
    "ALLOWED_CLASSES = {\n",
    "    \"Chest_Changes\",\n",
    "    \"Degenerative_Infectious\",\n",
    "    \"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\n",
    "    \"Lower_Density\",\n",
    "    \"Mediastinal_Changes\",\n",
    "    \"Normal\",\n",
    "    \"Obstructive\",\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # ===== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ =====\n",
    "    def extract_class(text: str):\n",
    "        \"\"\"‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return None\n",
    "        for cls in ALLOWED_CLASSES:\n",
    "            # ‡πÉ‡∏ä‡πâ regex ‡∏Ñ‡πâ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏ö‡∏ö exact (‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏≠‡∏ö ‡πÜ)\n",
    "            if re.search(rf\"\\b{re.escape(cls)}\\b\", text):\n",
    "                return cls\n",
    "        return None\n",
    "    # ===============================================\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á token IDs ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å pred ‡πÅ‡∏•‡∏∞ label\n",
    "    pred_classes = [extract_class(p) for p in preds]\n",
    "    label_classes = [extract_class(l) for l in labels]\n",
    "\n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy ‚Äî ‡∏ô‡∏±‡∏ö‡∏ñ‡∏π‡∏Å‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô\n",
    "    correct, total = 0, 0\n",
    "    for p_cls, l_cls in zip(pred_classes, label_classes):\n",
    "        if l_cls is not None:\n",
    "            total += 1\n",
    "            if p_cls == l_cls:\n",
    "                correct += 1\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BLEU score (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=[[p.split()] for p in preds],\n",
    "        references=[[[l.split()]] for l in labels]\n",
    "    )[\"bleu\"]\n",
    "\n",
    "    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ metric ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà\n",
    "    return {\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_bleu\": bleu_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5f243a",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 1 (val idx 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11bc2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "# ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ö‡∏ô GPU (Ampere+)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FastVisionModel.for_training(model)  # enable training\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        # ===== Training schedule =====\n",
    "        per_device_train_batch_size = 2,\n",
    "        per_device_eval_batch_size  = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        seed = 3407,\n",
    "\n",
    "        # ===== Eval =====\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 15,\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 2,\n",
    "\n",
    "        # ===== Precision / dtype =====\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        tf32 = True,\n",
    "\n",
    "        # ===== Optimization =====\n",
    "        learning_rate = 2e-4,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        max_grad_norm = 1.0,\n",
    "\n",
    "        # ===== Dataloader (‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) =====\n",
    "        dataloader_num_workers = 0,      # üëà ‡∏õ‡∏¥‡∏î multi-worker ‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ pickle\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "        # ‡∏≠‡∏¢‡πà‡∏≤‡πÉ‡∏™‡πà dataloader_prefetch_factor / persistent_workers ‡πÄ‡∏°‡∏∑‡πà‡∏≠ num_workers=0\n",
    "\n",
    "        # ===== Logging / Checkpoint IO =====\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "        save_strategy = \"no\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = False,\n",
    "\n",
    "        # ===== Output =====\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "        # ===== Vision finetuning (required) =====\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,            # üëà ‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ pickle ‡∏ï‡∏≠‡∏ô map/prepare (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "        max_seq_length = 1024,\n",
    "\n",
    "        # ===== Memory saver =====\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.predict_with_generate = True\n",
    "trainer.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3d2fb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "3.557 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7324d982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,310 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 13:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.270400</td>\n",
       "      <td>1.165263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.206600</td>\n",
       "      <td>0.927082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "941ce3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839.5235 seconds used for training.\n",
      "13.99 minutes used for training.\n",
      "Peak reserved memory = 7.084 GB.\n",
      "Peak reserved memory for training = 3.527 GB.\n",
      "Peak reserved memory % of max memory = 118.126 %.\n",
      "Peak reserved memory for training % of max memory = 58.813 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23d097da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for this fold   = 0.3909956455230713\n",
      "Final Eval Loss for this fold = 0.9271\n",
      "All metrics: {'train_runtime': 839.5235, 'train_samples_per_second': 0.286, 'train_steps_per_second': 0.036, 'total_flos': 2168626228371456.0, 'train_loss': 0.3909956455230713, 'epoch': 0.05568445475638051}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7815</td>\n",
       "      <td>1.165311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.7681</td>\n",
       "      <td>1.153866</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7487</td>\n",
       "      <td>1.209582</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7156</td>\n",
       "      <td>1.199007</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.6734</td>\n",
       "      <td>1.196590</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6292</td>\n",
       "      <td>1.181954</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5517</td>\n",
       "      <td>1.084888</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5200</td>\n",
       "      <td>1.191804</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.4831</td>\n",
       "      <td>1.363444</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.016705</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4486</td>\n",
       "      <td>1.345038</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.4208</td>\n",
       "      <td>1.520114</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>0.020418</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.4046</td>\n",
       "      <td>1.243079</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.3442</td>\n",
       "      <td>1.061498</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.024130</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.3328</td>\n",
       "      <td>1.127405</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.025986</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2704</td>\n",
       "      <td>0.745932</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>15</td>\n",
       "      <td>1.165263</td>\n",
       "      <td>302.8396</td>\n",
       "      <td>2.846</td>\n",
       "      <td>0.713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2968</td>\n",
       "      <td>0.607211</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.029698</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2844</td>\n",
       "      <td>0.532377</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2925</td>\n",
       "      <td>0.568851</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.033411</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.507077</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.035267</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1832</td>\n",
       "      <td>0.560650</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.2541</td>\n",
       "      <td>0.504224</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.038979</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.2142</td>\n",
       "      <td>0.516932</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.040835</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.2604</td>\n",
       "      <td>0.479376</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.042691</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.2536</td>\n",
       "      <td>0.486763</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.044548</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.2441</td>\n",
       "      <td>0.401344</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.2603</td>\n",
       "      <td>0.427427</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.048260</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1841</td>\n",
       "      <td>0.432257</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.050116</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.2173</td>\n",
       "      <td>0.439686</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.051972</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.2195</td>\n",
       "      <td>0.472439</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.2066</td>\n",
       "      <td>0.480109</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>30</td>\n",
       "      <td>0.927082</td>\n",
       "      <td>352.0101</td>\n",
       "      <td>2.449</td>\n",
       "      <td>0.614</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>839.5235</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2.168626e+15</td>\n",
       "      <td>0.390996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.7815   1.165311       0.000000  0.001856     1        NaN           NaN   \n",
       "1   0.7681   1.153866       0.000040  0.003712     2        NaN           NaN   \n",
       "2   0.7487   1.209582       0.000080  0.005568     3        NaN           NaN   \n",
       "3   0.7156   1.199007       0.000120  0.007425     4        NaN           NaN   \n",
       "4   0.6734   1.196590       0.000160  0.009281     5        NaN           NaN   \n",
       "5   0.6292   1.181954       0.000200  0.011137     6        NaN           NaN   \n",
       "6   0.5517   1.084888       0.000192  0.012993     7        NaN           NaN   \n",
       "7   0.5200   1.191804       0.000184  0.014849     8        NaN           NaN   \n",
       "8   0.4831   1.363444       0.000176  0.016705     9        NaN           NaN   \n",
       "9   0.4486   1.345038       0.000168  0.018561    10        NaN           NaN   \n",
       "10  0.4208   1.520114       0.000160  0.020418    11        NaN           NaN   \n",
       "11  0.4046   1.243079       0.000152  0.022274    12        NaN           NaN   \n",
       "12  0.3442   1.061498       0.000144  0.024130    13        NaN           NaN   \n",
       "13  0.3328   1.127405       0.000136  0.025986    14        NaN           NaN   \n",
       "14  0.2704   0.745932       0.000128  0.027842    15        NaN           NaN   \n",
       "15     NaN        NaN            NaN  0.027842    15   1.165263      302.8396   \n",
       "16  0.2968   0.607211       0.000120  0.029698    16        NaN           NaN   \n",
       "17  0.2844   0.532377       0.000112  0.031555    17        NaN           NaN   \n",
       "18  0.2925   0.568851       0.000104  0.033411    18        NaN           NaN   \n",
       "19  0.2660   0.507077       0.000096  0.035267    19        NaN           NaN   \n",
       "20  0.1832   0.560650       0.000088  0.037123    20        NaN           NaN   \n",
       "21  0.2541   0.504224       0.000080  0.038979    21        NaN           NaN   \n",
       "22  0.2142   0.516932       0.000072  0.040835    22        NaN           NaN   \n",
       "23  0.2604   0.479376       0.000064  0.042691    23        NaN           NaN   \n",
       "24  0.2536   0.486763       0.000056  0.044548    24        NaN           NaN   \n",
       "25  0.2441   0.401344       0.000048  0.046404    25        NaN           NaN   \n",
       "26  0.2603   0.427427       0.000040  0.048260    26        NaN           NaN   \n",
       "27  0.1841   0.432257       0.000032  0.050116    27        NaN           NaN   \n",
       "28  0.2173   0.439686       0.000024  0.051972    28        NaN           NaN   \n",
       "29  0.2195   0.472439       0.000016  0.053828    29        NaN           NaN   \n",
       "30  0.2066   0.480109       0.000008  0.055684    30        NaN           NaN   \n",
       "31     NaN        NaN            NaN  0.055684    30   0.927082      352.0101   \n",
       "32     NaN        NaN            NaN  0.055684    30        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                       NaN                    NaN            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                       NaN                    NaN            NaN   \n",
       "10                      NaN                    NaN            NaN   \n",
       "11                      NaN                    NaN            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                      NaN                    NaN            NaN   \n",
       "15                    2.846                  0.713            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                      NaN                    NaN            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                      NaN                    NaN            NaN   \n",
       "20                      NaN                    NaN            NaN   \n",
       "21                      NaN                    NaN            NaN   \n",
       "22                      NaN                    NaN            NaN   \n",
       "23                      NaN                    NaN            NaN   \n",
       "24                      NaN                    NaN            NaN   \n",
       "25                      NaN                    NaN            NaN   \n",
       "26                      NaN                    NaN            NaN   \n",
       "27                      NaN                    NaN            NaN   \n",
       "28                      NaN                    NaN            NaN   \n",
       "29                      NaN                    NaN            NaN   \n",
       "30                      NaN                    NaN            NaN   \n",
       "31                    2.449                  0.614            NaN   \n",
       "32                      NaN                    NaN       839.5235   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                       NaN                     NaN           NaN         NaN  \n",
       "21                       NaN                     NaN           NaN         NaN  \n",
       "22                       NaN                     NaN           NaN         NaN  \n",
       "23                       NaN                     NaN           NaN         NaN  \n",
       "24                       NaN                     NaN           NaN         NaN  \n",
       "25                       NaN                     NaN           NaN         NaN  \n",
       "26                       NaN                     NaN           NaN         NaN  \n",
       "27                       NaN                     NaN           NaN         NaN  \n",
       "28                       NaN                     NaN           NaN         NaN  \n",
       "29                       NaN                     NaN           NaN         NaN  \n",
       "30                       NaN                     NaN           NaN         NaN  \n",
       "31                       NaN                     NaN           NaN         NaN  \n",
       "32                     0.286                   0.036  2.168626e+15    0.390996  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss for this fold   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss for this fold = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c0878f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c652a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_idx0_20251005_193653\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_idx{idx_selected}_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41589881",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 2 (val idx 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f381b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# === Hyperparameter Set 1: \"Conservative LR + Cosine schedule + more steps\" ===\n",
    "# ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°:\n",
    "# - learning_rate: 2e-4 -> 1e-4 (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏∏‡∏ô‡πÅ‡∏£‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï)\n",
    "# - lr_scheduler_type: linear -> cosine (‡∏•‡∏î LR ‡πÅ‡∏ö‡∏ö‡πÇ‡∏Ñ‡πâ‡∏á‡∏ô‡∏¥‡πà‡∏°‡∏•‡∏á)\n",
    "# - warmup_steps: 5 -> 6 (‡∏≠‡∏∏‡πà‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "# - max_steps: 30 -> 60 (‡∏ù‡∏∂‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "# - optim: adamw_8bit -> adamw_torch (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏±‡∏ß optimizer)\n",
    "# - weight_decay: 0.01 -> 0.05 (regularization ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "# - max_grad_norm: 1.0 -> 0.5 (clip gradient ‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "# - output_dir ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô \"outputs_set1\"\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer_set1 = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        per_device_eval_batch_size  = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 6,                     # CHANGED\n",
    "        max_steps = 60,                       # CHANGED\n",
    "        seed = 3407,\n",
    "\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 30,                      # CHANGED\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 2,\n",
    "\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        tf32 = True,\n",
    "\n",
    "        learning_rate = 1e-4,                 # CHANGED\n",
    "        optim = \"adamw_torch\",                # CHANGED\n",
    "        weight_decay = 0.05,                  # CHANGED\n",
    "        lr_scheduler_type = \"cosine\",         # CHANGED\n",
    "        max_grad_norm = 0.5,                  # CHANGED\n",
    "\n",
    "        dataloader_num_workers = 0,\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "        save_strategy = \"no\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = False,\n",
    "\n",
    "        output_dir = \"outputs_set1\",          # CHANGED\n",
    "\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,\n",
    "        max_seq_length = 1024,\n",
    "\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_set1.predict_with_generate = True\n",
    "trainer_set1.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5fbe93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "2.465 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdacc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,310 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 16:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.282000</td>\n",
       "      <td>1.162117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.252100</td>\n",
       "      <td>0.932469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer_set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed072e99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1025.0976 seconds used for training.\n",
      "17.08 minutes used for training.\n",
      "Peak reserved memory = 6.764 GB.\n",
      "Peak reserved memory for training = 4.299 GB.\n",
      "Peak reserved memory % of max memory = 112.79 %.\n",
      "Peak reserved memory for training % of max memory = 71.686 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4611e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for this fold   = 0.4216022936006387\n",
      "Final Eval Loss for this fold = 0.9325\n",
      "All metrics: {'train_runtime': 1025.0976, 'train_samples_per_second': 0.468, 'train_steps_per_second': 0.059, 'total_flos': 4346869795356672.0, 'train_loss': 0.4216022936006387, 'epoch': 0.11136890951276102}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9793</td>\n",
       "      <td>2.768430</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9330</td>\n",
       "      <td>1.693358</td>\n",
       "      <td>1.666667e-05</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9404</td>\n",
       "      <td>1.802520</td>\n",
       "      <td>3.333333e-05</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9301</td>\n",
       "      <td>1.665295</td>\n",
       "      <td>5.000000e-05</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8811</td>\n",
       "      <td>1.510039</td>\n",
       "      <td>6.666667e-05</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.2353</td>\n",
       "      <td>0.440168</td>\n",
       "      <td>7.596123e-07</td>\n",
       "      <td>0.107657</td>\n",
       "      <td>58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.2149</td>\n",
       "      <td>0.447256</td>\n",
       "      <td>3.380821e-07</td>\n",
       "      <td>0.109513</td>\n",
       "      <td>59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.2521</td>\n",
       "      <td>0.526875</td>\n",
       "      <td>8.459209e-08</td>\n",
       "      <td>0.111369</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111369</td>\n",
       "      <td>60</td>\n",
       "      <td>0.932469</td>\n",
       "      <td>342.5282</td>\n",
       "      <td>2.517</td>\n",
       "      <td>0.631</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111369</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1025.0976</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.059</td>\n",
       "      <td>4.346870e+15</td>\n",
       "      <td>0.421602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.9793   2.768430   0.000000e+00  0.001856     1        NaN           NaN   \n",
       "1   0.9330   1.693358   1.666667e-05  0.003712     2        NaN           NaN   \n",
       "2   0.9404   1.802520   3.333333e-05  0.005568     3        NaN           NaN   \n",
       "3   0.9301   1.665295   5.000000e-05  0.007425     4        NaN           NaN   \n",
       "4   0.8811   1.510039   6.666667e-05  0.009281     5        NaN           NaN   \n",
       "..     ...        ...            ...       ...   ...        ...           ...   \n",
       "58  0.2353   0.440168   7.596123e-07  0.107657    58        NaN           NaN   \n",
       "59  0.2149   0.447256   3.380821e-07  0.109513    59        NaN           NaN   \n",
       "60  0.2521   0.526875   8.459209e-08  0.111369    60        NaN           NaN   \n",
       "61     NaN        NaN            NaN  0.111369    60   0.932469      342.5282   \n",
       "62     NaN        NaN            NaN  0.111369    60        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "..                      ...                    ...            ...   \n",
       "58                      NaN                    NaN            NaN   \n",
       "59                      NaN                    NaN            NaN   \n",
       "60                      NaN                    NaN            NaN   \n",
       "61                    2.517                  0.631            NaN   \n",
       "62                      NaN                    NaN      1025.0976   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "..                       ...                     ...           ...         ...  \n",
       "58                       NaN                     NaN           NaN         NaN  \n",
       "59                       NaN                     NaN           NaN         NaN  \n",
       "60                       NaN                     NaN           NaN         NaN  \n",
       "61                       NaN                     NaN           NaN         NaN  \n",
       "62                     0.468                   0.059  4.346870e+15    0.421602  \n",
       "\n",
       "[63 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer_set1.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss for this fold   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss for this fold = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9016d07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer_set1.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "275aba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_idx1_20251005_200456\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_idx{idx_selected}_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a83ba",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 3 (val idx 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba61e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# === Hyperparameter Set 2: \"Larger batch per device + Cosine-with-restarts + higher LR\" ===\n",
    "# ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°:\n",
    "# - per_device_train_batch_size: 2 -> 4 (‡πÄ‡∏û‡∏¥‡πà‡∏° batch ‡∏ï‡πà‡∏≠‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå)\n",
    "# - per_device_eval_batch_size: 4 -> 8 (‡πÄ‡∏û‡∏¥‡πà‡∏° batch ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô)\n",
    "# - gradient_accumulation_steps: 4 -> 2 (‡∏•‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ effective batch ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡πÄ‡∏î‡∏¥‡∏°)\n",
    "# - learning_rate: 2e-4 -> 3e-4 (‡∏•‡∏≠‡∏á LR ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "# - lr_scheduler_type: linear -> cosine_with_restarts (‡∏°‡∏µ‡∏ß‡∏±‡∏è‡∏à‡∏±‡∏Å‡∏£‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ)\n",
    "# - eval_steps: 15 -> 10 (‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ñ‡∏µ‡πà‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "# - eval_accumulation_steps: 2 -> 1 (‡∏•‡∏î memory ‡∏ä‡πà‡∏ß‡∏á eval)\n",
    "# - max_seq_length: 1024 -> 1536 (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö sequence ‡∏¢‡∏≤‡∏ß‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "# - output_dir ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô \"outputs_set2\"\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer_set2 = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 4,       # CHANGED\n",
    "        per_device_eval_batch_size  = 8,       # CHANGED\n",
    "        gradient_accumulation_steps = 2,       # CHANGED\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        seed = 3407,\n",
    "\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 10,                       # CHANGED\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 1,           # CHANGED\n",
    "\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        tf32 = True,\n",
    "\n",
    "        learning_rate = 3e-4,                  # CHANGED\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine_with_restarts\",  # CHANGED\n",
    "        max_grad_norm = 1.0,\n",
    "\n",
    "        dataloader_num_workers = 0,\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "        save_strategy = \"no\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = False,\n",
    "\n",
    "        output_dir = \"outputs_set2\",           # CHANGED\n",
    "\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,\n",
    "        max_seq_length = 1536,                 # CHANGED\n",
    "\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_set2.predict_with_generate = True\n",
    "trainer_set2.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213f2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "2.465 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6991ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,310 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 21:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>1.692101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.449400</td>\n",
       "      <td>0.978422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.397900</td>\n",
       "      <td>0.919924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer_set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de75fa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1293.5883 seconds used for training.\n",
      "21.56 minutes used for training.\n",
      "Peak reserved memory = 6.229 GB.\n",
      "Peak reserved memory for training = 3.764 GB.\n",
      "Peak reserved memory % of max memory = 103.869 %.\n",
      "Peak reserved memory for training % of max memory = 62.765 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c0859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for this fold   = 0.8425906846920649\n",
      "Final Eval Loss for this fold = 0.9199\n",
      "All metrics: {'train_runtime': 1293.5883, 'train_samples_per_second': 0.186, 'train_steps_per_second': 0.023, 'total_flos': 2221270703652864.0, 'train_loss': 0.8425906846920649, 'epoch': 0.055658627087198514}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.9508</td>\n",
       "      <td>2.189928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.8756</td>\n",
       "      <td>1.640249</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.003711</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.8407</td>\n",
       "      <td>1.623699</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.005566</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.7417</td>\n",
       "      <td>1.646072</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.007421</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.5755</td>\n",
       "      <td>1.212267</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.4218</td>\n",
       "      <td>1.112914</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.011132</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.2413</td>\n",
       "      <td>1.046245</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.1501</td>\n",
       "      <td>1.038751</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.014842</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0508</td>\n",
       "      <td>1.097420</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.016698</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.9309</td>\n",
       "      <td>1.304464</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.018553</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018553</td>\n",
       "      <td>10</td>\n",
       "      <td>1.692101</td>\n",
       "      <td>285.9598</td>\n",
       "      <td>3.014</td>\n",
       "      <td>0.755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.8596</td>\n",
       "      <td>1.512290</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.8133</td>\n",
       "      <td>1.086744</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.022263</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.6453</td>\n",
       "      <td>1.875852</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.6125</td>\n",
       "      <td>0.799398</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.025974</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.5165</td>\n",
       "      <td>1.060555</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.027829</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.5950</td>\n",
       "      <td>0.557036</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.029685</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5449</td>\n",
       "      <td>0.558005</td>\n",
       "      <td>0.000178</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.5268</td>\n",
       "      <td>0.481877</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.033395</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.454615</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.035250</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.4494</td>\n",
       "      <td>0.498645</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.037106</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037106</td>\n",
       "      <td>20</td>\n",
       "      <td>0.978422</td>\n",
       "      <td>311.8073</td>\n",
       "      <td>2.765</td>\n",
       "      <td>0.693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.4580</td>\n",
       "      <td>0.412544</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.038961</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.4581</td>\n",
       "      <td>0.454481</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.4763</td>\n",
       "      <td>0.473357</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.042672</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.4771</td>\n",
       "      <td>0.442390</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.044527</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.4419</td>\n",
       "      <td>0.432314</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.046382</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.4861</td>\n",
       "      <td>0.411542</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.048237</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.3837</td>\n",
       "      <td>0.393029</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.050093</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.3990</td>\n",
       "      <td>0.466537</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.4565</td>\n",
       "      <td>0.532723</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.053803</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.437556</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>30</td>\n",
       "      <td>0.919924</td>\n",
       "      <td>485.5803</td>\n",
       "      <td>1.775</td>\n",
       "      <td>0.445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055659</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1293.5883</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.023</td>\n",
       "      <td>2.221271e+15</td>\n",
       "      <td>0.842591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   1.9508   2.189928       0.000000  0.001855     1        NaN           NaN   \n",
       "1   1.8756   1.640249       0.000060  0.003711     2        NaN           NaN   \n",
       "2   1.8407   1.623699       0.000120  0.005566     3        NaN           NaN   \n",
       "3   1.7417   1.646072       0.000180  0.007421     4        NaN           NaN   \n",
       "4   1.5755   1.212267       0.000240  0.009276     5        NaN           NaN   \n",
       "5   1.4218   1.112914       0.000300  0.011132     6        NaN           NaN   \n",
       "6   1.2413   1.046245       0.000299  0.012987     7        NaN           NaN   \n",
       "7   1.1501   1.038751       0.000295  0.014842     8        NaN           NaN   \n",
       "8   1.0508   1.097420       0.000289  0.016698     9        NaN           NaN   \n",
       "9   0.9309   1.304464       0.000281  0.018553    10        NaN           NaN   \n",
       "10     NaN        NaN            NaN  0.018553    10   1.692101      285.9598   \n",
       "11  0.8596   1.512290       0.000271  0.020408    11        NaN           NaN   \n",
       "12  0.8133   1.086744       0.000259  0.022263    12        NaN           NaN   \n",
       "13  0.6453   1.875852       0.000246  0.024119    13        NaN           NaN   \n",
       "14  0.6125   0.799398       0.000230  0.025974    14        NaN           NaN   \n",
       "15  0.5165   1.060555       0.000214  0.027829    15        NaN           NaN   \n",
       "16  0.5950   0.557036       0.000196  0.029685    16        NaN           NaN   \n",
       "17  0.5449   0.558005       0.000178  0.031540    17        NaN           NaN   \n",
       "18  0.5268   0.481877       0.000159  0.033395    18        NaN           NaN   \n",
       "19  0.5005   0.454615       0.000141  0.035250    19        NaN           NaN   \n",
       "20  0.4494   0.498645       0.000122  0.037106    20        NaN           NaN   \n",
       "21     NaN        NaN            NaN  0.037106    20   0.978422      311.8073   \n",
       "22  0.4580   0.412544       0.000104  0.038961    21        NaN           NaN   \n",
       "23  0.4581   0.454481       0.000086  0.040816    22        NaN           NaN   \n",
       "24  0.4763   0.473357       0.000070  0.042672    23        NaN           NaN   \n",
       "25  0.4771   0.442390       0.000054  0.044527    24        NaN           NaN   \n",
       "26  0.4419   0.432314       0.000041  0.046382    25        NaN           NaN   \n",
       "27  0.4861   0.411542       0.000029  0.048237    26        NaN           NaN   \n",
       "28  0.3837   0.393029       0.000019  0.050093    27        NaN           NaN   \n",
       "29  0.3990   0.466537       0.000011  0.051948    28        NaN           NaN   \n",
       "30  0.4565   0.532723       0.000005  0.053803    29        NaN           NaN   \n",
       "31  0.3979   0.437556       0.000001  0.055659    30        NaN           NaN   \n",
       "32     NaN        NaN            NaN  0.055659    30   0.919924      485.5803   \n",
       "33     NaN        NaN            NaN  0.055659    30        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                       NaN                    NaN            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                       NaN                    NaN            NaN   \n",
       "10                    3.014                  0.755            NaN   \n",
       "11                      NaN                    NaN            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                      NaN                    NaN            NaN   \n",
       "15                      NaN                    NaN            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                      NaN                    NaN            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                      NaN                    NaN            NaN   \n",
       "20                      NaN                    NaN            NaN   \n",
       "21                    2.765                  0.693            NaN   \n",
       "22                      NaN                    NaN            NaN   \n",
       "23                      NaN                    NaN            NaN   \n",
       "24                      NaN                    NaN            NaN   \n",
       "25                      NaN                    NaN            NaN   \n",
       "26                      NaN                    NaN            NaN   \n",
       "27                      NaN                    NaN            NaN   \n",
       "28                      NaN                    NaN            NaN   \n",
       "29                      NaN                    NaN            NaN   \n",
       "30                      NaN                    NaN            NaN   \n",
       "31                      NaN                    NaN            NaN   \n",
       "32                    1.775                  0.445            NaN   \n",
       "33                      NaN                    NaN      1293.5883   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                       NaN                     NaN           NaN         NaN  \n",
       "21                       NaN                     NaN           NaN         NaN  \n",
       "22                       NaN                     NaN           NaN         NaN  \n",
       "23                       NaN                     NaN           NaN         NaN  \n",
       "24                       NaN                     NaN           NaN         NaN  \n",
       "25                       NaN                     NaN           NaN         NaN  \n",
       "26                       NaN                     NaN           NaN         NaN  \n",
       "27                       NaN                     NaN           NaN         NaN  \n",
       "28                       NaN                     NaN           NaN         NaN  \n",
       "29                       NaN                     NaN           NaN         NaN  \n",
       "30                       NaN                     NaN           NaN         NaN  \n",
       "31                       NaN                     NaN           NaN         NaN  \n",
       "32                       NaN                     NaN           NaN         NaN  \n",
       "33                     0.186                   0.023  2.221271e+15    0.842591  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer_set2.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss for this fold   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss for this fold = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51ab3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer_set2.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ffd8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_idx2_20251005_203527\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_idx{idx_selected}_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c465a",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 4 (val idx 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de90d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# === Hyperparameter Set 3: \"Long-context focus + polynomial decay + no weight decay\" ===\n",
    "# ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°:\n",
    "# - per_device_train_batch_size: 2 -> 1 (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ VRAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö seq ‡∏¢‡∏≤‡∏ß)\n",
    "# - per_device_eval_batch_size: 4 -> 2 (‡∏•‡∏î eval batch ‡∏ï‡∏≤‡∏° VRAM)\n",
    "# - gradient_accumulation_steps: 4 -> 8 (‡∏£‡∏±‡∏Å‡∏©‡∏≤ effective batch ‡∏£‡∏ß‡∏°)\n",
    "# - max_seq_length: 1024 -> 2048 (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö context ‡∏¢‡∏≤‡∏ß)\n",
    "# - learning_rate: 2e-4 -> 1.5e-4 (‡∏•‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏±‡∏ö context ‡∏¢‡∏≤‡∏ß)\n",
    "# - lr_scheduler_type: linear -> polynomial (‡πÇ‡∏Ñ‡πâ‡∏á‡∏•‡∏î‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á/‡∏Å‡∏≥‡∏•‡∏±‡∏á n)\n",
    "# - weight_decay: 0.01 -> 0.0 (‡∏õ‡∏¥‡∏î WD ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á)\n",
    "# - warmup_steps: 5 -> 8 (‡∏≠‡∏∏‡πà‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°)\n",
    "# - max_steps: 30 -> 40 (‡∏ù‡∏∂‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "# - output_dir ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô \"outputs_set3\"\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer_set3 = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,       # CHANGED\n",
    "        per_device_eval_batch_size  = 2,       # CHANGED\n",
    "        gradient_accumulation_steps = 8,       # CHANGED\n",
    "        warmup_steps = 8,                       # CHANGED\n",
    "        max_steps = 40,                         # CHANGED\n",
    "        seed = 3407,\n",
    "\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 15,\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 2,\n",
    "\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        tf32 = True,\n",
    "\n",
    "        learning_rate = 1.5e-4,                # CHANGED\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,                     # CHANGED\n",
    "        lr_scheduler_type = \"polynomial\",       # CHANGED\n",
    "        max_grad_norm = 1.0,\n",
    "\n",
    "        dataloader_num_workers = 0,\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "        save_strategy = \"no\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = False,\n",
    "\n",
    "        output_dir = \"outputs_set3\",            # CHANGED\n",
    "\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,\n",
    "        max_seq_length = 2048,                  # CHANGED\n",
    "\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_set3.predict_with_generate = True\n",
    "trainer_set3.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0baa8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "2.465 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2669dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,310 | Num Epochs = 1 | Total steps = 40\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 14:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>1.954199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.134000</td>\n",
       "      <td>1.050711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer_set3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24ba642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "916.6479 seconds used for training.\n",
      "15.28 minutes used for training.\n",
      "Peak reserved memory = 4.965 GB.\n",
      "Peak reserved memory for training = 2.5 GB.\n",
      "Peak reserved memory % of max memory = 82.791 %.\n",
      "Peak reserved memory for training % of max memory = 41.688 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cf05e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for this fold   = 0.23881329149007796\n",
      "Final Eval Loss for this fold = 1.0507\n",
      "All metrics: {'train_runtime': 916.6479, 'train_samples_per_second': 0.349, 'train_steps_per_second': 0.044, 'total_flos': 2755722936225792.0, 'train_loss': 0.23881329149007796, 'epoch': 0.07424593967517401}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4939</td>\n",
       "      <td>2.119401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4832</td>\n",
       "      <td>1.784608</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4537</td>\n",
       "      <td>1.632744</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.005568</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4624</td>\n",
       "      <td>1.640307</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.007425</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4355</td>\n",
       "      <td>1.394392</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4283</td>\n",
       "      <td>1.372211</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4018</td>\n",
       "      <td>1.191673</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.012993</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3773</td>\n",
       "      <td>1.116006</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3590</td>\n",
       "      <td>1.100156</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.016705</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3477</td>\n",
       "      <td>1.165369</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.3366</td>\n",
       "      <td>1.332893</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.020418</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.3055</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.022274</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.2960</td>\n",
       "      <td>1.042512</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.024130</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.2677</td>\n",
       "      <td>1.091922</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.025986</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2489</td>\n",
       "      <td>1.219185</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>15</td>\n",
       "      <td>1.954199</td>\n",
       "      <td>303.2076</td>\n",
       "      <td>2.843</td>\n",
       "      <td>1.421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2506</td>\n",
       "      <td>1.257625</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.029698</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2296</td>\n",
       "      <td>1.290220</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.031555</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2115</td>\n",
       "      <td>1.468686</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.033411</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.2112</td>\n",
       "      <td>1.402801</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.035267</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1963</td>\n",
       "      <td>1.267999</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1838</td>\n",
       "      <td>1.224362</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.038979</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1787</td>\n",
       "      <td>0.920854</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.040835</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1633</td>\n",
       "      <td>0.821201</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.042691</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1614</td>\n",
       "      <td>1.040113</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.044548</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1467</td>\n",
       "      <td>0.854544</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.1516</td>\n",
       "      <td>0.610913</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.048260</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.577563</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.050116</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.547961</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.051972</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1353</td>\n",
       "      <td>0.548133</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.053828</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.1340</td>\n",
       "      <td>0.485498</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>30</td>\n",
       "      <td>1.050711</td>\n",
       "      <td>297.8620</td>\n",
       "      <td>2.894</td>\n",
       "      <td>1.447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.1150</td>\n",
       "      <td>0.490932</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.057541</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1323</td>\n",
       "      <td>0.489635</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.059397</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.1216</td>\n",
       "      <td>0.471630</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.061253</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.1318</td>\n",
       "      <td>0.505190</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.063109</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.424099</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.1172</td>\n",
       "      <td>0.527152</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.066821</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.1346</td>\n",
       "      <td>0.542193</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.068677</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.521950</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.070534</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.450024</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.072390</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.1321</td>\n",
       "      <td>0.431473</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.074246</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074246</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>916.6479</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.044</td>\n",
       "      <td>2.755723e+15</td>\n",
       "      <td>0.238813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.4939   2.119401       0.000000  0.001856     1        NaN           NaN   \n",
       "1   0.4832   1.784608       0.000019  0.003712     2        NaN           NaN   \n",
       "2   0.4537   1.632744       0.000037  0.005568     3        NaN           NaN   \n",
       "3   0.4624   1.640307       0.000056  0.007425     4        NaN           NaN   \n",
       "4   0.4355   1.394392       0.000075  0.009281     5        NaN           NaN   \n",
       "5   0.4283   1.372211       0.000094  0.011137     6        NaN           NaN   \n",
       "6   0.4018   1.191673       0.000112  0.012993     7        NaN           NaN   \n",
       "7   0.3773   1.116006       0.000131  0.014849     8        NaN           NaN   \n",
       "8   0.3590   1.100156       0.000150  0.016705     9        NaN           NaN   \n",
       "9   0.3477   1.165369       0.000145  0.018561    10        NaN           NaN   \n",
       "10  0.3366   1.332893       0.000141  0.020418    11        NaN           NaN   \n",
       "11  0.3055   0.996680       0.000136  0.022274    12        NaN           NaN   \n",
       "12  0.2960   1.042512       0.000131  0.024130    13        NaN           NaN   \n",
       "13  0.2677   1.091922       0.000127  0.025986    14        NaN           NaN   \n",
       "14  0.2489   1.219185       0.000122  0.027842    15        NaN           NaN   \n",
       "15     NaN        NaN            NaN  0.027842    15   1.954199      303.2076   \n",
       "16  0.2506   1.257625       0.000117  0.029698    16        NaN           NaN   \n",
       "17  0.2296   1.290220       0.000113  0.031555    17        NaN           NaN   \n",
       "18  0.2115   1.468686       0.000108  0.033411    18        NaN           NaN   \n",
       "19  0.2112   1.402801       0.000103  0.035267    19        NaN           NaN   \n",
       "20  0.1963   1.267999       0.000098  0.037123    20        NaN           NaN   \n",
       "21  0.1838   1.224362       0.000094  0.038979    21        NaN           NaN   \n",
       "22  0.1787   0.920854       0.000089  0.040835    22        NaN           NaN   \n",
       "23  0.1633   0.821201       0.000084  0.042691    23        NaN           NaN   \n",
       "24  0.1614   1.040113       0.000080  0.044548    24        NaN           NaN   \n",
       "25  0.1467   0.854544       0.000075  0.046404    25        NaN           NaN   \n",
       "26  0.1516   0.610913       0.000070  0.048260    26        NaN           NaN   \n",
       "27  0.1238   0.577563       0.000066  0.050116    27        NaN           NaN   \n",
       "28  0.1374   0.547961       0.000061  0.051972    28        NaN           NaN   \n",
       "29  0.1353   0.548133       0.000056  0.053828    29        NaN           NaN   \n",
       "30  0.1340   0.485498       0.000052  0.055684    30        NaN           NaN   \n",
       "31     NaN        NaN            NaN  0.055684    30   1.050711      297.8620   \n",
       "32  0.1150   0.490932       0.000047  0.057541    31        NaN           NaN   \n",
       "33  0.1323   0.489635       0.000042  0.059397    32        NaN           NaN   \n",
       "34  0.1216   0.471630       0.000038  0.061253    33        NaN           NaN   \n",
       "35  0.1318   0.505190       0.000033  0.063109    34        NaN           NaN   \n",
       "36  0.1195   0.424099       0.000028  0.064965    35        NaN           NaN   \n",
       "37  0.1172   0.527152       0.000024  0.066821    36        NaN           NaN   \n",
       "38  0.1346   0.542193       0.000019  0.068677    37        NaN           NaN   \n",
       "39  0.1111   0.521950       0.000014  0.070534    38        NaN           NaN   \n",
       "40  0.1245   0.450024       0.000009  0.072390    39        NaN           NaN   \n",
       "41  0.1321   0.431473       0.000005  0.074246    40        NaN           NaN   \n",
       "42     NaN        NaN            NaN  0.074246    40        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                       NaN                    NaN            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                       NaN                    NaN            NaN   \n",
       "10                      NaN                    NaN            NaN   \n",
       "11                      NaN                    NaN            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                      NaN                    NaN            NaN   \n",
       "15                    2.843                  1.421            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                      NaN                    NaN            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                      NaN                    NaN            NaN   \n",
       "20                      NaN                    NaN            NaN   \n",
       "21                      NaN                    NaN            NaN   \n",
       "22                      NaN                    NaN            NaN   \n",
       "23                      NaN                    NaN            NaN   \n",
       "24                      NaN                    NaN            NaN   \n",
       "25                      NaN                    NaN            NaN   \n",
       "26                      NaN                    NaN            NaN   \n",
       "27                      NaN                    NaN            NaN   \n",
       "28                      NaN                    NaN            NaN   \n",
       "29                      NaN                    NaN            NaN   \n",
       "30                      NaN                    NaN            NaN   \n",
       "31                    2.894                  1.447            NaN   \n",
       "32                      NaN                    NaN            NaN   \n",
       "33                      NaN                    NaN            NaN   \n",
       "34                      NaN                    NaN            NaN   \n",
       "35                      NaN                    NaN            NaN   \n",
       "36                      NaN                    NaN            NaN   \n",
       "37                      NaN                    NaN            NaN   \n",
       "38                      NaN                    NaN            NaN   \n",
       "39                      NaN                    NaN            NaN   \n",
       "40                      NaN                    NaN            NaN   \n",
       "41                      NaN                    NaN            NaN   \n",
       "42                      NaN                    NaN       916.6479   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                       NaN                     NaN           NaN         NaN  \n",
       "21                       NaN                     NaN           NaN         NaN  \n",
       "22                       NaN                     NaN           NaN         NaN  \n",
       "23                       NaN                     NaN           NaN         NaN  \n",
       "24                       NaN                     NaN           NaN         NaN  \n",
       "25                       NaN                     NaN           NaN         NaN  \n",
       "26                       NaN                     NaN           NaN         NaN  \n",
       "27                       NaN                     NaN           NaN         NaN  \n",
       "28                       NaN                     NaN           NaN         NaN  \n",
       "29                       NaN                     NaN           NaN         NaN  \n",
       "30                       NaN                     NaN           NaN         NaN  \n",
       "31                       NaN                     NaN           NaN         NaN  \n",
       "32                       NaN                     NaN           NaN         NaN  \n",
       "33                       NaN                     NaN           NaN         NaN  \n",
       "34                       NaN                     NaN           NaN         NaN  \n",
       "35                       NaN                     NaN           NaN         NaN  \n",
       "36                       NaN                     NaN           NaN         NaN  \n",
       "37                       NaN                     NaN           NaN         NaN  \n",
       "38                       NaN                     NaN           NaN         NaN  \n",
       "39                       NaN                     NaN           NaN         NaN  \n",
       "40                       NaN                     NaN           NaN         NaN  \n",
       "41                       NaN                     NaN           NaN         NaN  \n",
       "42                     0.349                   0.044  2.755723e+15    0.238813  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer_set3.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss for this fold   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss for this fold = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01f041cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer_set3.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1fec553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_idx3_20251005_205609\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_idx{idx_selected}_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba29bf7",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 5 (val idx 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b42291a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# === Hyperparameter Set 4: \"Constant-with-warmup + save best on eval_loss\" ===\n",
    "# ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°:\n",
    "# - lr_scheduler_type: linear -> constant_with_warmup (LR ‡∏Ñ‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏á warmup)\n",
    "# - weight_decay: 0.01 -> 0.02 (‡πÄ‡∏û‡∏¥‡πà‡∏° regularization ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "# - max_grad_norm: 1.0 -> 0.8 (clip gradient ‡πÄ‡∏Ç‡πâ‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "# - logging_steps: 1 -> 5 (‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà logging)\n",
    "# - save_strategy: \"no\" -> \"steps\" ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° save_steps=40 (‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡πà‡∏ß‡∏á ‡πÜ)\n",
    "# - load_best_model_at_end: False -> True (‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà eval ‡∏î‡∏µ‡∏™‡∏∏‡∏î‡∏ï‡∏≠‡∏ô‡∏à‡∏ö)\n",
    "# - eval_strategy: \"steps\" ‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡∏à‡∏∞‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö save ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏∑‡∏≠‡∏Å best\n",
    "# - output_dir ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô \"outputs_set4\"\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer_set4 = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        per_device_eval_batch_size  = 4,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 80,                         # CHANGED (‡∏ù‡∏∂‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "        seed = 3407,\n",
    "\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 20,                        # CHANGED (‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏´‡πà‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô)\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 2,\n",
    "\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        tf32 = True,\n",
    "\n",
    "        learning_rate = 2e-4,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.02,                    # CHANGED\n",
    "        lr_scheduler_type = \"constant_with_warmup\",  # CHANGED\n",
    "        max_grad_norm = 0.8,                    # CHANGED\n",
    "\n",
    "        dataloader_num_workers = 0,\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 5,                      # CHANGED\n",
    "        save_strategy = \"steps\",                # CHANGED\n",
    "        save_steps = 40,                        # CHANGED\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = True,          # CHANGED\n",
    "\n",
    "        output_dir = \"outputs_set4\",            # CHANGED\n",
    "\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,\n",
    "        max_seq_length = 1024,\n",
    "\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_set4.predict_with_generate = True\n",
    "trainer_set4.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7278a322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "2.465 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f682a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,310 | Num Epochs = 1 | Total steps = 80\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 26:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.988202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.672817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.528003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.124500</td>\n",
       "      <td>0.454099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer_set4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39709020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625.3629 seconds used for training.\n",
      "27.09 minutes used for training.\n",
      "Peak reserved memory = 6.029 GB.\n",
      "Peak reserved memory for training = 3.564 GB.\n",
      "Peak reserved memory % of max memory = 100.534 %.\n",
      "Peak reserved memory for training % of max memory = 59.43 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e224b96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for this fold   = 0.26264787167310716\n",
      "Final Eval Loss for this fold = 0.4541\n",
      "All metrics: {'train_runtime': 1625.3629, 'train_samples_per_second': 0.394, 'train_steps_per_second': 0.049, 'total_flos': 5772828410167296.0, 'train_loss': 0.26264787167310716, 'epoch': 0.14849187935034802}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9197</td>\n",
       "      <td>1.193786</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>0.009281</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6565</td>\n",
       "      <td>1.140536</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.018561</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4486</td>\n",
       "      <td>1.099614</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.027842</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2863</td>\n",
       "      <td>0.562006</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.037123</td>\n",
       "      <td>20</td>\n",
       "      <td>0.988202</td>\n",
       "      <td>274.4897</td>\n",
       "      <td>3.140</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.2332</td>\n",
       "      <td>0.500872</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.2088</td>\n",
       "      <td>0.580975</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.055684</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1928</td>\n",
       "      <td>0.443820</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1701</td>\n",
       "      <td>0.612711</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.074246</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074246</td>\n",
       "      <td>40</td>\n",
       "      <td>0.672817</td>\n",
       "      <td>277.2335</td>\n",
       "      <td>3.109</td>\n",
       "      <td>0.779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.1615</td>\n",
       "      <td>0.568115</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.083527</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.1581</td>\n",
       "      <td>0.557316</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.092807</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.1413</td>\n",
       "      <td>0.633464</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.102088</td>\n",
       "      <td>55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.1315</td>\n",
       "      <td>0.633270</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.111369</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.111369</td>\n",
       "      <td>60</td>\n",
       "      <td>0.528003</td>\n",
       "      <td>280.2679</td>\n",
       "      <td>3.076</td>\n",
       "      <td>0.771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.673839</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.120650</td>\n",
       "      <td>65</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.1195</td>\n",
       "      <td>0.557120</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.129930</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.1164</td>\n",
       "      <td>0.644996</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.139211</td>\n",
       "      <td>75</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.669169</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>80</td>\n",
       "      <td>0.454099</td>\n",
       "      <td>313.3010</td>\n",
       "      <td>2.751</td>\n",
       "      <td>0.689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.148492</td>\n",
       "      <td>80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1625.3629</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.049</td>\n",
       "      <td>5.772828e+15</td>\n",
       "      <td>0.262648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.9197   1.193786        0.00016  0.009281     5        NaN           NaN   \n",
       "1   0.6565   1.140536        0.00020  0.018561    10        NaN           NaN   \n",
       "2   0.4486   1.099614        0.00020  0.027842    15        NaN           NaN   \n",
       "3   0.2863   0.562006        0.00020  0.037123    20        NaN           NaN   \n",
       "4      NaN        NaN            NaN  0.037123    20   0.988202      274.4897   \n",
       "5   0.2332   0.500872        0.00020  0.046404    25        NaN           NaN   \n",
       "6   0.2088   0.580975        0.00020  0.055684    30        NaN           NaN   \n",
       "7   0.1928   0.443820        0.00020  0.064965    35        NaN           NaN   \n",
       "8   0.1701   0.612711        0.00020  0.074246    40        NaN           NaN   \n",
       "9      NaN        NaN            NaN  0.074246    40   0.672817      277.2335   \n",
       "10  0.1615   0.568115        0.00020  0.083527    45        NaN           NaN   \n",
       "11  0.1581   0.557316        0.00020  0.092807    50        NaN           NaN   \n",
       "12  0.1413   0.633464        0.00020  0.102088    55        NaN           NaN   \n",
       "13  0.1315   0.633270        0.00020  0.111369    60        NaN           NaN   \n",
       "14     NaN        NaN            NaN  0.111369    60   0.528003      280.2679   \n",
       "15  0.1335   0.673839        0.00020  0.120650    65        NaN           NaN   \n",
       "16  0.1195   0.557120        0.00020  0.129930    70        NaN           NaN   \n",
       "17  0.1164   0.644996        0.00020  0.139211    75        NaN           NaN   \n",
       "18  0.1245   0.669169        0.00020  0.148492    80        NaN           NaN   \n",
       "19     NaN        NaN            NaN  0.148492    80   0.454099      313.3010   \n",
       "20     NaN        NaN            NaN  0.148492    80        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                     3.140                  0.787            NaN   \n",
       "5                       NaN                    NaN            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                     3.109                  0.779            NaN   \n",
       "10                      NaN                    NaN            NaN   \n",
       "11                      NaN                    NaN            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                    3.076                  0.771            NaN   \n",
       "15                      NaN                    NaN            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                      NaN                    NaN            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                    2.751                  0.689            NaN   \n",
       "20                      NaN                    NaN      1625.3629   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                     0.394                   0.049  5.772828e+15    0.262648  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer_set4.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss for this fold   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss for this fold = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8ca2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer_set4.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f86ab10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_idx4_20251005_212819\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_idx{idx_selected}_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b0824",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 6 (val idx 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef1b13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# === Hyperparameter Set 5 (fixed): \"BF16 steady + smaller LR + more warmup\" ===\n",
    "# ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡∏Ç‡∏≠‡∏á Set5:\n",
    "# - precision: ‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß -> fp16=False, bf16=True, tf32=True  (FIX)\n",
    "# - per_device_train_batch_size: 2 -> 3\n",
    "# - per_device_eval_batch_size: 4 -> 6\n",
    "# - learning_rate: 2e-4 -> 5e-5\n",
    "# - warmup_steps: 5 -> 10\n",
    "# - lr_scheduler_type: linear -> constant_with_warmup\n",
    "# - eval_steps: 15 -> 20\n",
    "# - output_dir: \"outputs_set5\"\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer_set5 = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        # ===== Training schedule =====\n",
    "        per_device_train_batch_size = 3,        # CHANGED\n",
    "        per_device_eval_batch_size  = 6,        # CHANGED\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 10,                      # CHANGED\n",
    "        max_steps = 30,\n",
    "        seed = 3407,\n",
    "\n",
    "        # ===== Eval =====\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 20,                        # CHANGED\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 2,\n",
    "\n",
    "        # ===== Precision / dtype (‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô bf16) =====\n",
    "        fp16 = False,                           # FIXED\n",
    "        bf16 = True,                            # FIXED\n",
    "        tf32 = True,                            # FIXED\n",
    "\n",
    "        # ===== Optimization =====\n",
    "        learning_rate = 5e-5,                   # CHANGED\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"constant_with_warmup\",  # CHANGED\n",
    "        max_grad_norm = 1.0,\n",
    "\n",
    "        # ===== Dataloader =====\n",
    "        dataloader_num_workers = 0,\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "\n",
    "        # ===== Logging / Checkpoint IO =====\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "        save_strategy = \"no\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = False,\n",
    "\n",
    "        # ===== Output =====\n",
    "        output_dir = \"outputs_set5\",            # CHANGED\n",
    "\n",
    "        # ===== Vision finetuning (required) =====\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,\n",
    "        max_seq_length = 1024,\n",
    "\n",
    "        # ===== Memory saver =====\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer_set5.predict_with_generate = True\n",
    "trainer_set5.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a817b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "2.465 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ac5c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,310 | Num Epochs = 1 | Total steps = 30\n",
      "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 4 x 1) = 12\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 10:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>2.537837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer_set5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c97475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628.2143 seconds used for training.\n",
      "10.47 minutes used for training.\n",
      "Peak reserved memory = 9.244 GB.\n",
      "Peak reserved memory for training = 6.779 GB.\n",
      "Peak reserved memory % of max memory = 154.144 %.\n",
      "Peak reserved memory for training % of max memory = 113.04 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9fcc397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss for this fold   = 0.7400482495625814\n",
      "Final Eval Loss for this fold = 2.5378\n",
      "All metrics: {'train_runtime': 628.2143, 'train_samples_per_second': 0.573, 'train_steps_per_second': 0.048, 'total_flos': 3299228011450368.0, 'train_loss': 0.7400482495625814, 'epoch': 0.08350730688935282}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9629</td>\n",
       "      <td>1.921401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9372</td>\n",
       "      <td>1.684279</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.005567</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9183</td>\n",
       "      <td>1.672936</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9235</td>\n",
       "      <td>1.694051</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.011134</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.8962</td>\n",
       "      <td>1.560194</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.013918</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.9170</td>\n",
       "      <td>1.627645</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.016701</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.9428</td>\n",
       "      <td>1.653763</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.019485</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.9051</td>\n",
       "      <td>1.598700</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.022269</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.8613</td>\n",
       "      <td>1.412274</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.025052</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.8608</td>\n",
       "      <td>1.432127</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.8263</td>\n",
       "      <td>1.238013</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.030619</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.8177</td>\n",
       "      <td>1.353917</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.033403</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.8001</td>\n",
       "      <td>1.213635</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.036186</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.7653</td>\n",
       "      <td>1.124826</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.038970</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.7434</td>\n",
       "      <td>1.110905</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.041754</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.7084</td>\n",
       "      <td>1.092526</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.044537</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.7038</td>\n",
       "      <td>1.140933</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.047321</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6822</td>\n",
       "      <td>1.190125</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.050104</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.6808</td>\n",
       "      <td>1.119310</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.052888</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.6440</td>\n",
       "      <td>1.095637</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.055672</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055672</td>\n",
       "      <td>20</td>\n",
       "      <td>2.537837</td>\n",
       "      <td>363.0368</td>\n",
       "      <td>2.374</td>\n",
       "      <td>0.397</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.6455</td>\n",
       "      <td>1.130511</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.058455</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.6122</td>\n",
       "      <td>1.068394</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.061239</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.6013</td>\n",
       "      <td>1.044455</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.064022</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.5900</td>\n",
       "      <td>1.096743</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.066806</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.5867</td>\n",
       "      <td>1.145221</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.069589</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.5540</td>\n",
       "      <td>1.057950</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.072373</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.5411</td>\n",
       "      <td>1.095483</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.5634</td>\n",
       "      <td>1.207075</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.077940</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.4967</td>\n",
       "      <td>1.187515</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.080724</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.5136</td>\n",
       "      <td>1.197687</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.083507</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.083507</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>628.2143</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.048</td>\n",
       "      <td>3.299228e+15</td>\n",
       "      <td>0.740048</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.9629   1.921401       0.000000  0.002784     1        NaN           NaN   \n",
       "1   0.9372   1.684279       0.000005  0.005567     2        NaN           NaN   \n",
       "2   0.9183   1.672936       0.000010  0.008351     3        NaN           NaN   \n",
       "3   0.9235   1.694051       0.000015  0.011134     4        NaN           NaN   \n",
       "4   0.8962   1.560194       0.000020  0.013918     5        NaN           NaN   \n",
       "5   0.9170   1.627645       0.000025  0.016701     6        NaN           NaN   \n",
       "6   0.9428   1.653763       0.000030  0.019485     7        NaN           NaN   \n",
       "7   0.9051   1.598700       0.000035  0.022269     8        NaN           NaN   \n",
       "8   0.8613   1.412274       0.000040  0.025052     9        NaN           NaN   \n",
       "9   0.8608   1.432127       0.000045  0.027836    10        NaN           NaN   \n",
       "10  0.8263   1.238013       0.000050  0.030619    11        NaN           NaN   \n",
       "11  0.8177   1.353917       0.000050  0.033403    12        NaN           NaN   \n",
       "12  0.8001   1.213635       0.000050  0.036186    13        NaN           NaN   \n",
       "13  0.7653   1.124826       0.000050  0.038970    14        NaN           NaN   \n",
       "14  0.7434   1.110905       0.000050  0.041754    15        NaN           NaN   \n",
       "15  0.7084   1.092526       0.000050  0.044537    16        NaN           NaN   \n",
       "16  0.7038   1.140933       0.000050  0.047321    17        NaN           NaN   \n",
       "17  0.6822   1.190125       0.000050  0.050104    18        NaN           NaN   \n",
       "18  0.6808   1.119310       0.000050  0.052888    19        NaN           NaN   \n",
       "19  0.6440   1.095637       0.000050  0.055672    20        NaN           NaN   \n",
       "20     NaN        NaN            NaN  0.055672    20   2.537837      363.0368   \n",
       "21  0.6455   1.130511       0.000050  0.058455    21        NaN           NaN   \n",
       "22  0.6122   1.068394       0.000050  0.061239    22        NaN           NaN   \n",
       "23  0.6013   1.044455       0.000050  0.064022    23        NaN           NaN   \n",
       "24  0.5900   1.096743       0.000050  0.066806    24        NaN           NaN   \n",
       "25  0.5867   1.145221       0.000050  0.069589    25        NaN           NaN   \n",
       "26  0.5540   1.057950       0.000050  0.072373    26        NaN           NaN   \n",
       "27  0.5411   1.095483       0.000050  0.075157    27        NaN           NaN   \n",
       "28  0.5634   1.207075       0.000050  0.077940    28        NaN           NaN   \n",
       "29  0.4967   1.187515       0.000050  0.080724    29        NaN           NaN   \n",
       "30  0.5136   1.197687       0.000050  0.083507    30        NaN           NaN   \n",
       "31     NaN        NaN            NaN  0.083507    30        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                       NaN                    NaN            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                       NaN                    NaN            NaN   \n",
       "10                      NaN                    NaN            NaN   \n",
       "11                      NaN                    NaN            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                      NaN                    NaN            NaN   \n",
       "15                      NaN                    NaN            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                      NaN                    NaN            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                      NaN                    NaN            NaN   \n",
       "20                    2.374                  0.397            NaN   \n",
       "21                      NaN                    NaN            NaN   \n",
       "22                      NaN                    NaN            NaN   \n",
       "23                      NaN                    NaN            NaN   \n",
       "24                      NaN                    NaN            NaN   \n",
       "25                      NaN                    NaN            NaN   \n",
       "26                      NaN                    NaN            NaN   \n",
       "27                      NaN                    NaN            NaN   \n",
       "28                      NaN                    NaN            NaN   \n",
       "29                      NaN                    NaN            NaN   \n",
       "30                      NaN                    NaN            NaN   \n",
       "31                      NaN                    NaN       628.2143   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                       NaN                     NaN           NaN         NaN  \n",
       "21                       NaN                     NaN           NaN         NaN  \n",
       "22                       NaN                     NaN           NaN         NaN  \n",
       "23                       NaN                     NaN           NaN         NaN  \n",
       "24                       NaN                     NaN           NaN         NaN  \n",
       "25                       NaN                     NaN           NaN         NaN  \n",
       "26                       NaN                     NaN           NaN         NaN  \n",
       "27                       NaN                     NaN           NaN         NaN  \n",
       "28                       NaN                     NaN           NaN         NaN  \n",
       "29                       NaN                     NaN           NaN         NaN  \n",
       "30                       NaN                     NaN           NaN         NaN  \n",
       "31                     0.573                   0.048  3.299228e+15    0.740048  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer_set5.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss for this fold   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss for this fold = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f5b28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "# df = pd.DataFrame(trainer_set4.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9ba7e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_idx5_20251005_214520\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_idx{idx_selected}_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a423d",
   "metadata": {},
   "source": [
    "### 6. ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Eval Accuracy ‡πÇ‡∏î‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Manual Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c2ef2",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 1 (val idx 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475c480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 23:25:35.496000 19828 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ===== 0) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + LoRA (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) =====\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "model.load_adapter(\"lora_model_val_idx0_20251005_193653\")\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0714d567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n",
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 862\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")\n",
    "\n",
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "idx_selected = 0                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(val_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be3930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 1, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 2, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 3, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 4, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 5, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 6, true_class : Normal, pred_class : Normal\n",
      "id: 7, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 8, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 9, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 10, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 11, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 12, true_class : Chest_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 13, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 14, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 15, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 16, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 17, true_class : Higher_Density, pred_class : Normal\n",
      "id: 18, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 19, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 20, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 21, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 22, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 23, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 24, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 25, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 26, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 27, true_class : Mediastinal_Changes, pred_class : Normal\n",
      "id: 28, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 29, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.2000 (6/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_idx0.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = val_selected.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô ALLOWED_CLASSES)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_idx{idx_selected}.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53728b6a",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 2 (val idx 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41cff022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ===== 0) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + LoRA (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) =====\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "model.load_adapter(\"lora_model_val_idx1_20251005_200456\")\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81292b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n",
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 862\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")\n",
    "\n",
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "idx_selected = 1                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(val_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "205d53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Normal, pred_class : Normal\n",
      "id: 1, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 2, true_class : Inflammatory_Pneumonia, pred_class : Normal\n",
      "id: 3, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 4, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 5, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 6, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 7, true_class : Lower_Density, pred_class : Mediastinal_Changes\n",
      "id: 8, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 9, true_class : Mediastinal_Changes, pred_class : Mediastinal_Changes\n",
      "id: 10, true_class : Chest_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 11, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 12, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 13, true_class : Mediastinal_Changes, pred_class : Mediastinal_Changes\n",
      "id: 14, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 15, true_class : Higher_Density, pred_class : Normal\n",
      "id: 16, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 17, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 18, true_class : Normal, pred_class : Normal\n",
      "id: 19, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 20, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 21, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 22, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 23, true_class : Lower_Density, pred_class : Normal\n",
      "id: 24, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 25, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 26, true_class : Chest_Changes, pred_class : Normal\n",
      "id: 27, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 28, true_class : Chest_Changes, pred_class : Higher_Density\n",
      "id: 29, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.2000 (6/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_idx1.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = val_selected.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô ALLOWED_CLASSES)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_idx{idx_selected}.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac306c6",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 3 (val idx 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "858c2a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 23:51:33.595000 21900 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ===== 0) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + LoRA (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) =====\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "model.load_adapter(\"lora_model_val_idx2_20251005_203527\")\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2ae135f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n",
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "['Higher_Density', 'Inflammatory_Pneumonia', 'Obstructive', 'Obstructive', 'Inflammatory_Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")\n",
    "\n",
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "idx_selected = 2                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(val_selected[0:5][\"__class__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec8a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 1, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 2, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 3, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 4, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 5, true_class : Chest_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 6, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 7, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 8, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 9, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 10, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 11, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 12, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 13, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 14, true_class : Higher_Density, pred_class : Normal\n",
      "id: 15, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 16, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 17, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 18, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 19, true_class : Higher_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 20, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 21, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 22, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 23, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 24, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 25, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 26, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 27, true_class : Higher_Density, pred_class : Normal\n",
      "id: 28, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 29, true_class : Normal, pred_class : Normal\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.2667 (8/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_idx2.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = val_selected.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô CLASS_SET)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_idx{idx_selected}.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afd1034",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 4 (val idx 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25315dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ===== 0) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + LoRA (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) =====\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "model.load_adapter(\"lora_model_val_idx3_20251005_205609\")\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc4da5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n",
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "['Normal', 'Higher_Density', 'Degenerative_Infectious', 'Inflammatory_Pneumonia', 'Inflammatory_Pneumonia']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")\n",
    "\n",
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "idx_selected = 3                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(val_selected[0:5][\"__class__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a2c6be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Normal, pred_class : Normal\n",
      "id: 1, true_class : Higher_Density, pred_class : Normal\n",
      "id: 2, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 3, true_class : Inflammatory_Pneumonia, pred_class : Normal\n",
      "id: 4, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 5, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 6, true_class : Obstructive, pred_class : Mediastinal_Changes\n",
      "id: 7, true_class : Mediastinal_Changes, pred_class : Mediastinal_Changes\n",
      "id: 8, true_class : Normal, pred_class : Normal\n",
      "id: 9, true_class : Lower_Density, pred_class : Normal\n",
      "id: 10, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 11, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "id: 12, true_class : Mediastinal_Changes, pred_class : Normal\n",
      "id: 13, true_class : Lower_Density, pred_class : Normal\n",
      "id: 14, true_class : Lower_Density, pred_class : Normal\n",
      "id: 15, true_class : Mediastinal_Changes, pred_class : Mediastinal_Changes\n",
      "id: 16, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 17, true_class : Lower_Density, pred_class : Inflammatory_Pneumonia\n",
      "id: 18, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 19, true_class : Normal, pred_class : Normal\n",
      "id: 20, true_class : Mediastinal_Changes, pred_class : Normal\n",
      "id: 21, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 22, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 23, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 24, true_class : Lower_Density, pred_class : Mediastinal_Changes\n",
      "id: 25, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 26, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 27, true_class : Inflammatory_Pneumonia, pred_class : Inflammatory_Pneumonia\n",
      "id: 28, true_class : Chest_Changes, pred_class : Higher_Density\n",
      "id: 29, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.3333 (10/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_idx3.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = val_selected.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô CLASS_SET)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_idx{idx_selected}.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feea486",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 5 (val idx 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9290f7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1006 00:15:29.226000 16176 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ===== 0) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + LoRA (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) =====\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "model.load_adapter(\"lora_model_val_idx4_20251005_212819\")\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4657703b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n",
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "['Inflammatory_Pneumonia', 'Normal', 'Inflammatory_Pneumonia', 'Obstructive', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")\n",
    "\n",
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "idx_selected = 4                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(val_selected[0:5][\"__class__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9523b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 1, true_class : Normal, pred_class : Degenerative_Infectious\n",
      "id: 2, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 3, true_class : Obstructive, pred_class : Degenerative_Infectious\n",
      "id: 4, true_class : Normal, pred_class : Degenerative_Infectious\n",
      "id: 5, true_class : Normal, pred_class : Degenerative_Infectious\n",
      "id: 6, true_class : Higher_Density, pred_class : Degenerative_Infectious\n",
      "id: 7, true_class : Lower_Density, pred_class : Degenerative_Infectious\n",
      "id: 8, true_class : Obstructive, pred_class : Degenerative_Infectious\n",
      "id: 9, true_class : Normal, pred_class : Degenerative_Infectious\n",
      "id: 10, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 11, true_class : Lower_Density, pred_class : Degenerative_Infectious\n",
      "id: 12, true_class : Obstructive, pred_class : Degenerative_Infectious\n",
      "id: 13, true_class : Chest_Changes, pred_class : Degenerative_Infectious\n",
      "id: 14, true_class : Mediastinal_Changes, pred_class : Degenerative_Infectious\n",
      "id: 15, true_class : Obstructive, pred_class : Degenerative_Infectious\n",
      "id: 16, true_class : Obstructive, pred_class : Degenerative_Infectious\n",
      "id: 17, true_class : Chest_Changes, pred_class : Degenerative_Infectious\n",
      "id: 18, true_class : Chest_Changes, pred_class : Degenerative_Infectious\n",
      "id: 19, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 20, true_class : Chest_Changes, pred_class : Degenerative_Infectious\n",
      "id: 21, true_class : Degenerative_Infectious, pred_class : Degenerative_Infectious\n",
      "id: 22, true_class : Degenerative_Infectious, pred_class : Degenerative_Infectious\n",
      "id: 23, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 24, true_class : Higher_Density, pred_class : Degenerative_Infectious\n",
      "id: 25, true_class : Normal, pred_class : Degenerative_Infectious\n",
      "id: 26, true_class : Lower_Density, pred_class : Degenerative_Infectious\n",
      "id: 27, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 28, true_class : Inflammatory_Pneumonia, pred_class : Degenerative_Infectious\n",
      "id: 29, true_class : Chest_Changes, pred_class : Degenerative_Infectious\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.0667 (2/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_idx4.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = val_selected.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô CLASS_SET)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_idx{idx_selected}.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079354bb",
   "metadata": {},
   "source": [
    "#### ü•¨ Round 6 (val idx 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd70201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2_5_VLForConditionalGeneration(\n",
       "  (model): Qwen2_5_VLModel(\n",
       "    (visual): Qwen2_5_VisionTransformerPretrainedModel(\n",
       "      (patch_embed): Qwen2_5_VisionPatchEmbed(\n",
       "        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)\n",
       "      )\n",
       "      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()\n",
       "      (blocks): ModuleList(\n",
       "        (0-31): 32 x Qwen2_5_VLVisionBlock(\n",
       "          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "          (attn): Qwen2_5_VLVisionAttention(\n",
       "            (qkv): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3840, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3840, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "          )\n",
       "          (mlp): Qwen2_5_VLMLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=1280, out_features=3420, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=1280, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=3420, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=3420, out_features=1280, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=3420, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=1280, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (merger): Qwen2_5_VLPatchMerger(\n",
       "        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=5120, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=2048, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (language_model): Qwen2_5_VLTextModel(\n",
       "      (embed_tokens): Embedding(151936, 2048, padding_idx=151654)\n",
       "      (layers): ModuleList(\n",
       "        (0-35): 36 x Qwen2_5_VLDecoderLayer(\n",
       "          (self_attn): Qwen2_5_VLAttention(\n",
       "            (q_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (k_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (v_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (o_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (up_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=11008, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (down_proj): lora.Linear4bit(\n",
       "              (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Identity()\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=11008, out_features=16, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen2_5_VLRotaryEmbedding()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "from unsloth import FastVisionModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# ===== 0) ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• + LoRA (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) =====\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit=True,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    ")\n",
    "model.load_adapter(\"lora_model_val_idx5_20251005_214520\")\n",
    "FastVisionModel.for_inference(model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a432f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 5,172 (85.00%)\n",
      "Test set size      : 913 (15.00%)\n",
      "Total train_hf : 5172\n",
      "Each fold size : 862\n",
      "['Chest_Changes', 'Normal', 'Normal', 'Lower_Density', 'Lower_Density']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")\n",
    "\n",
    "n = len(train_hf)\n",
    "k = 6\n",
    "fold_size = n // k\n",
    "\n",
    "folds = []\n",
    "for i in range(k):\n",
    "    start = i * fold_size\n",
    "    end = (i + 1) * fold_size if i < k - 1 else n\n",
    "    fold = train_hf.select(range(start, end))\n",
    "    folds.append(fold)\n",
    "\n",
    "idx_selected = 5                        # ‚Üê ‡πÅ‡∏Å‡πâ‡πÄ‡∏•‡∏Ç‡πÉ‡∏ô‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£\n",
    "val_selected = folds[idx_selected]            \n",
    "\n",
    "print(f\"Total train_hf : {n}\")\n",
    "print(f\"Each fold size : {fold_size}\")\n",
    "print(val_selected[0:5][\"__class__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d874ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Chest_Changes, pred_class : Normal\n",
      "id: 1, true_class : Normal, pred_class : Normal\n",
      "id: 2, true_class : Normal, pred_class : Normal\n",
      "id: 3, true_class : Lower_Density, pred_class : Normal\n",
      "id: 4, true_class : Lower_Density, pred_class : Normal\n",
      "id: 5, true_class : Normal, pred_class : Normal\n",
      "id: 6, true_class : Normal, pred_class : Normal\n",
      "id: 7, true_class : Normal, pred_class : Chest_Changes\n",
      "id: 8, true_class : Degenerative_Infectious, pred_class : None\n",
      "id: 9, true_class : Lower_Density, pred_class : Normal\n",
      "id: 10, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 11, true_class : Chest_Changes, pred_class : Chest_Changes\n",
      "id: 12, true_class : Inflammatory_Pneumonia, pred_class : Normal\n",
      "id: 13, true_class : Mediastinal_Changes, pred_class : Chest_Changes\n",
      "id: 14, true_class : Mediastinal_Changes, pred_class : Chest_Changes\n",
      "id: 15, true_class : Higher_Density, pred_class : None\n",
      "id: 16, true_class : Chest_Changes, pred_class : Normal\n",
      "id: 17, true_class : Normal, pred_class : Normal\n",
      "id: 18, true_class : Degenerative_Infectious, pred_class : None\n",
      "id: 19, true_class : Lower_Density, pred_class : Normal\n",
      "id: 20, true_class : Chest_Changes, pred_class : Higher_Density\n",
      "id: 21, true_class : Lower_Density, pred_class : None\n",
      "id: 22, true_class : Inflammatory_Pneumonia, pred_class : Normal\n",
      "id: 23, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 24, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 25, true_class : Higher_Density, pred_class : Chest_Changes\n",
      "id: 26, true_class : Inflammatory_Pneumonia, pred_class : Normal\n",
      "id: 27, true_class : Normal, pred_class : Normal\n",
      "id: 28, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 29, true_class : Chest_Changes, pred_class : Chest_Changes\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.2667 (8/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_idx5.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = val_selected.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô CLASS_SET)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_idx{idx_selected}.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad6ca1",
   "metadata": {},
   "source": [
    "### 7. ‡∏ó‡∏≥‡∏ï‡∏≤‡∏£‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏° Mean ¬± SD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79948e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Fold    Training Loss        Eval Loss    Eval Accuracy\n",
      "0          1           0.3909           0.9271              0.2\n",
      "1          2           0.4216           0.9325              0.2\n",
      "2          3           0.8425           0.9199           0.2667\n",
      "3          4           0.2388           1.0507           0.3333\n",
      "4          5           0.2626           0.4541           0.0667\n",
      "5          6             0.74           2.5378           0.2667\n",
      "6  Mean ¬± SD  0.4827 ¬± 0.2294  1.1370 ¬± 0.6544  0.2222 ¬± 0.0831\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "eval_loss  = [0.9271, 0.9325, 0.9199, 1.0507, 0.4541, 2.5378]\n",
    "eval_acc   = [0.2000, 0.2000, 0.2667, 0.3333, 0.0667, 0.2667]\n",
    "train_loss = [0.3909, 0.4216, 0.8425, 0.2388, 0.2626, 0.7400]\n",
    "\n",
    "data = {\n",
    "    \"Fold\": [1, 2, 3, 4, 5, 6],\n",
    "    \"Training Loss\": train_loss,\n",
    "    \"Eval Loss\": eval_loss,\n",
    "    \"Eval Accuracy\": eval_acc\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "mean_row = {\n",
    "    \"Fold\": \"Mean ¬± SD\",\n",
    "    \"Training Loss\": f\"{np.mean(train_loss):.4f} ¬± {np.std(train_loss):.4f}\",\n",
    "    \"Eval Loss\": f\"{np.mean(eval_loss):.4f} ¬± {np.std(eval_loss):.4f}\",\n",
    "    \"Eval Accuracy\": f\"{np.mean(eval_acc):.4f} ¬± {np.std(eval_acc):.4f}\",\n",
    "}\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame([mean_row])], ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa6927f",
   "metadata": {},
   "source": [
    "‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏Å‡∏ß‡πà‡∏≤\n",
    "\n",
    "‚úÖ Fold ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏™‡∏∏‡∏î‡∏Ñ‡∏∑‡∏≠ fold ‡∏ó‡∏µ‡πà‡∏°‡∏µ Eval Loss ‡∏ï‡πà‡∏≥ ‡πÅ‡∏•‡∏∞ Eval Accuracy ‡∏™‡∏π‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ö Train Loss ‡πÑ‡∏°‡πà‡∏ï‡πà‡∏≤‡∏á‡∏°‡∏≤‡∏Å‡∏à‡∏≤‡∏Å Eval Loss\n",
    "\n",
    "| Fold  | Eval Loss | Eval Accuracy | Comment                                    |\n",
    "| ----- | --------- | ------------- | ------------------------------------------ |\n",
    "| 1     | 0.927     | 0.200         | ‡∏Å‡∏•‡∏≤‡∏á ‡πÜ                                     |\n",
    "| 2     | 0.933     | 0.200         | ‡∏Å‡∏•‡∏≤‡∏á ‡πÜ                                     |\n",
    "| **3** | **0.920** | **0.267**     | ‡∏î‡∏µ (loss ‡∏ï‡πà‡∏≥ + accuracy ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô)            |\n",
    "| **4** | **1.051** | **0.333**     | ‡∏î‡∏µ‡∏™‡∏∏‡∏î‡πÉ‡∏ô accuracy ‡πÅ‡∏ï‡πà eval loss ‡∏¢‡∏±‡∏á‡∏™‡∏π‡∏á      |\n",
    "| 5     | 0.454     | 0.067         | loss ‡∏ï‡πà‡∏≥‡∏™‡∏∏‡∏î‡πÅ‡∏ï‡πà accuracy ‡∏ï‡πà‡∏≥‡∏°‡∏≤‡∏Å (‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à‡∏ú‡∏¥‡∏î) |\n",
    "| 6     | 2.538     | 0.267         | loss ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å (‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏°‡πà‡∏î‡∏µ)                |\n",
    "\n",
    "‡∏™‡∏£‡∏∏‡∏õ Fold ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î: Fold 4 (‡∏£‡∏≠‡∏á‡∏Ñ‡∏∑‡∏≠ Fold 3)\n",
    "- Accuracy ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î ‚Üí 0.3333\n",
    "- Train Loss ‡∏ï‡πà‡∏≥ (0.2388) ‚Üí ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏î‡∏µ\n",
    "- Eval Loss (1.05) ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢ ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1 SD\n",
    "- ‡πÅ‡∏õ‡∏•‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• generalize ‡πÑ‡∏î‡πâ‡∏û‡∏≠‡∏™‡∏°‡∏Ñ‡∏ß‡∏£ ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà overfit ‡∏à‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ\n",
    "\n",
    "‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤: Fold 3\n",
    "- Accuracy ‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤ (0.2667)\n",
    "- Eval Loss ‡∏ï‡πà‡∏≥ (0.9199)\n",
    "- Train Loss ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ ‚Üí ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ fit ‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‚Üí ‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á ‚Äú‡∏™‡∏°‡∏î‡∏∏‡∏•‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850c42a",
   "metadata": {},
   "source": [
    "‡∏Å‡∏≤‡∏£‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°‡∏°‡∏µ Accuracy ‡∏ï‡πà‡∏≥ (‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢ 22%) ‚Üí ‡∏¢‡∏±‡∏á‡∏ï‡πâ‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á\n",
    "- ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á Eval Loss ‡∏™‡∏π‡∏á (¬±0.65) ‚Üí ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á folds\n",
    "- Fold 4 ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• ‚Äú‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÑ‡∏î‡πâ‡∏ö‡∏≤‡∏á‡∏™‡πà‡∏ß‡∏ô‚Äù ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d21ca74",
   "metadata": {},
   "source": [
    "### 8. Train Final Model (‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢)\n",
    "\n",
    "- ‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å ‚Äúconfig ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‚Äù ‡πÅ‡∏•‡πâ‡∏ß ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡∏ó‡∏∏‡∏Å fold) ‡∏°‡∏≤‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏´‡∏ô‡∏∂‡πà‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb2aec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size : 6,085\n",
      "Train set size     : 4,292 (70.53%)\n",
      "Val set size       : 880 (14.46%)\n",
      "Test set size      : 913 (15.00%)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "splits = hf.train_test_split(test_size=0.15, seed=42, shuffle=True)\n",
    "\n",
    "all_train_hf = splits[\"train\"]\n",
    "blind_test_hf = splits[\"test\"]       # blind test\n",
    "\n",
    "# 85\n",
    "vt = all_train_hf.train_test_split(test_size=0.17, seed=42, shuffle=True)\n",
    "train_hf = vt[\"train\"]\n",
    "val_hf = vt[\"test\"]\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏∏‡∏î\n",
    "print(f\"Total dataset size : {len(hf):,}\")\n",
    "print(f\"Train set size     : {len(train_hf):,} ({len(train_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Val set size       : {len(val_hf):,} ({len(val_hf)/len(hf)*100:.2f}%)\")\n",
    "print(f\"Test set size      : {len(blind_test_hf):,} ({len(blind_test_hf)/len(hf)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d368c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.'},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=450x450>}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Class: Degenerative_Infectious\\nExplanation: The pattern of bilateral hilar lymphadenopathy (sarcoidosis) aligns with bilateral, symmetric hilar enlargement with perihilar reticulonodular changes (sarcoidosis). Possibly representing Sarcoidosis. The lymphadenopathy is notable for its symmetric distribution. Perihilar interstitial thickening is also observed.'}]}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    cls_name = sample[\"__class__\"]\n",
    "    description = sample[\"text\"]\n",
    "\n",
    "    answer = f\"Class: {cls_name}\\nExplanation: {description}\"\n",
    "    \n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        {\"role\" : \"assistant\", \"content\" : [\n",
    "            {\"type\" : \"text\", \"text\" : answer} ]\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\" : conversation}\n",
    "\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in train_hf]\n",
    "\n",
    "converted_dataset_val = [convert_to_conversation(sample) for sample in val_hf]\n",
    "\n",
    "converted_dataset_test = [convert_to_conversation(sample) for sample in blind_test_hf]\n",
    "\n",
    "converted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aa03115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "# ‡πÇ‡∏´‡∏•‡∏î metric BLEU ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "bleu = load(\"bleu\")\n",
    "\n",
    "# ‡∏£‡∏≤‡∏¢‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï\n",
    "ALLOWED_CLASSES = {\n",
    "    \"Chest_Changes\",\n",
    "    \"Degenerative_Infectious\",\n",
    "    \"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\n",
    "    \"Lower_Density\",\n",
    "    \"Mediastinal_Changes\",\n",
    "    \"Normal\",\n",
    "    \"Obstructive\",\n",
    "}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    # ===== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏¢‡πà‡∏≠‡∏¢‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™ =====\n",
    "    def extract_class(text: str):\n",
    "        \"\"\"‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return None\n",
    "        for cls in ALLOWED_CLASSES:\n",
    "            # ‡πÉ‡∏ä‡πâ regex ‡∏Ñ‡πâ‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡πÅ‡∏ö‡∏ö exact (‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡∏£‡∏≠‡∏ö ‡πÜ)\n",
    "            if re.search(rf\"\\b{re.escape(cls)}\\b\", text):\n",
    "                return cls\n",
    "        return None\n",
    "    # ===============================================\n",
    "\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á token IDs ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ‡∏î‡∏∂‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å pred ‡πÅ‡∏•‡∏∞ label\n",
    "    pred_classes = [extract_class(p) for p in preds]\n",
    "    label_classes = [extract_class(l) for l in labels]\n",
    "\n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì accuracy ‚Äî ‡∏ô‡∏±‡∏ö‡∏ñ‡∏π‡∏Å‡∏ñ‡πâ‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô\n",
    "    correct, total = 0, 0\n",
    "    for p_cls, l_cls in zip(pred_classes, label_classes):\n",
    "        if l_cls is not None:\n",
    "            total += 1\n",
    "            if p_cls == l_cls:\n",
    "                correct += 1\n",
    "    acc = correct / total if total > 0 else 0.0\n",
    "\n",
    "    # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì BLEU score (‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=[[p.split()] for p in preds],\n",
    "        references=[[[l.split()]] for l in labels]\n",
    "    )[\"bleu\"]\n",
    "\n",
    "    # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ metric ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏π‡πà\n",
    "    return {\n",
    "        \"eval_accuracy\": acc,\n",
    "        \"eval_bleu\": bleu_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3d50bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Model does not have a default image size - using 512\n"
     ]
    }
   ],
   "source": [
    "# === Hyperparameter Set 3: \"Long-context focus + polynomial decay + no weight decay\" ===\n",
    "# ‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏î‡∏¥‡∏°:\n",
    "# - per_device_train_batch_size: 2 -> 1 (‡πÄ‡∏ú‡∏∑‡πà‡∏≠ VRAM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö seq ‡∏¢‡∏≤‡∏ß)\n",
    "# - per_device_eval_batch_size: 4 -> 2 (‡∏•‡∏î eval batch ‡∏ï‡∏≤‡∏° VRAM)\n",
    "# - gradient_accumulation_steps: 4 -> 8 (‡∏£‡∏±‡∏Å‡∏©‡∏≤ effective batch ‡∏£‡∏ß‡∏°)\n",
    "# - max_seq_length: 1024 -> 2048 (‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö context ‡∏¢‡∏≤‡∏ß)\n",
    "# - learning_rate: 2e-4 -> 1.5e-4 (‡∏•‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏±‡∏ö context ‡∏¢‡∏≤‡∏ß)\n",
    "# - lr_scheduler_type: linear -> polynomial (‡πÇ‡∏Ñ‡πâ‡∏á‡∏•‡∏î‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á/‡∏Å‡∏≥‡∏•‡∏±‡∏á n)\n",
    "# - weight_decay: 0.01 -> 0.0 (‡∏õ‡∏¥‡∏î WD ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≤‡∏á)\n",
    "# - warmup_steps: 5 -> 8 (‡∏≠‡∏∏‡πà‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°)\n",
    "# - max_steps: 30 -> 40 (‡∏ù‡∏∂‡∏Å‡∏ô‡∏≤‡∏ô‡∏Ç‡∏∂‡πâ‡∏ô‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢)\n",
    "# - output_dir ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô \"outputs_set3\"\n",
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset = converted_dataset_val,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,       # CHANGED\n",
    "        per_device_eval_batch_size  = 2,       # CHANGED\n",
    "        gradient_accumulation_steps = 8,       # CHANGED\n",
    "        warmup_steps = 8,                       # CHANGED\n",
    "        max_steps = 40,                         # CHANGED\n",
    "        seed = 3407,\n",
    "\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 15,\n",
    "        prediction_loss_only = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        eval_accumulation_steps = 2,\n",
    "\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "        tf32 = True,\n",
    "\n",
    "        learning_rate = 1.5e-4,                # CHANGED\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.0,                     # CHANGED\n",
    "        lr_scheduler_type = \"polynomial\",       # CHANGED\n",
    "        max_grad_norm = 1.0,\n",
    "\n",
    "        dataloader_num_workers = 0,\n",
    "        dataloader_pin_memory = True,\n",
    "        dataloader_drop_last = False,\n",
    "\n",
    "        report_to = [\"tensorboard\"],\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "        save_strategy = \"no\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = False,\n",
    "\n",
    "        output_dir = \"outputs\",            # CHANGED\n",
    "\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 1,\n",
    "        max_seq_length = 2048,                  # CHANGED\n",
    "\n",
    "        gradient_checkpointing = True,\n",
    "    ),\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "trainer.predict_with_generate = True\n",
    "trainer.gen_kwargs = {\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"do_sample\": False,\n",
    "    \"return_dict_in_generate\": False,\n",
    "    \"output_scores\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368db10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 4050 Laptop GPU. Max memory = 5.997 GB.\n",
      "2.465 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c53f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,292 | Num Epochs = 1 | Total steps = 40\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 8\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 8 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,084,928 of 3,795,707,904 (1.08% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 18:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.261300</td>\n",
       "      <td>1.949615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.138300</td>\n",
       "      <td>1.046528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen2_5_VLForConditionalGeneration does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00f97a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138.9442 seconds used for training.\n",
      "18.98 minutes used for training.\n",
      "Peak reserved memory = 4.939 GB.\n",
      "Peak reserved memory for training = 2.474 GB.\n",
      "Peak reserved memory % of max memory = 82.358 %.\n",
      "Peak reserved memory for training % of max memory = 41.254 %.\n"
     ]
    }
   ],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "271bd806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss   = 0.23811891842633487\n",
      "Final Eval Loss = 1.0465\n",
      "All metrics: {'train_runtime': 1138.9442, 'train_samples_per_second': 0.281, 'train_steps_per_second': 0.035, 'total_flos': 2781501585162240.0, 'train_loss': 0.23811891842633487, 'epoch': 0.07455731593662628}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>grad_norm</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>eval_loss</th>\n",
       "      <th>eval_runtime</th>\n",
       "      <th>eval_samples_per_second</th>\n",
       "      <th>eval_steps_per_second</th>\n",
       "      <th>train_runtime</th>\n",
       "      <th>train_samples_per_second</th>\n",
       "      <th>train_steps_per_second</th>\n",
       "      <th>total_flos</th>\n",
       "      <th>train_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4614</td>\n",
       "      <td>1.675442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001864</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.4720</td>\n",
       "      <td>1.648422</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.4780</td>\n",
       "      <td>1.745430</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.005592</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4447</td>\n",
       "      <td>1.534928</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4412</td>\n",
       "      <td>1.506474</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.009320</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.4309</td>\n",
       "      <td>1.384470</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.011184</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.4024</td>\n",
       "      <td>1.212762</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>0.013048</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3757</td>\n",
       "      <td>1.072563</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.014911</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3602</td>\n",
       "      <td>1.059663</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.016775</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.3207</td>\n",
       "      <td>1.018710</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.3042</td>\n",
       "      <td>0.986104</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.020503</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.2915</td>\n",
       "      <td>1.168620</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.2965</td>\n",
       "      <td>1.099148</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.024231</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.2722</td>\n",
       "      <td>1.118678</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>0.026095</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.2613</td>\n",
       "      <td>1.096027</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.027959</td>\n",
       "      <td>15</td>\n",
       "      <td>1.949615</td>\n",
       "      <td>411.4148</td>\n",
       "      <td>2.139</td>\n",
       "      <td>1.069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.2521</td>\n",
       "      <td>1.228678</td>\n",
       "      <td>0.000117</td>\n",
       "      <td>0.029823</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.2327</td>\n",
       "      <td>1.295628</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.031687</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.2291</td>\n",
       "      <td>1.552722</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.033551</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.1929</td>\n",
       "      <td>1.489771</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.035415</td>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.1929</td>\n",
       "      <td>1.344504</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.037279</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.1726</td>\n",
       "      <td>1.119166</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.039143</td>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.1656</td>\n",
       "      <td>1.016779</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.041007</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.1709</td>\n",
       "      <td>0.805474</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>0.042870</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.904257</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.044734</td>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.1621</td>\n",
       "      <td>0.690987</td>\n",
       "      <td>0.000075</td>\n",
       "      <td>0.046598</td>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.1504</td>\n",
       "      <td>0.627642</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.048462</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1526</td>\n",
       "      <td>1.395633</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.050326</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1198</td>\n",
       "      <td>0.568994</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.547417</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.055918</td>\n",
       "      <td>30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055918</td>\n",
       "      <td>30</td>\n",
       "      <td>1.046528</td>\n",
       "      <td>328.4405</td>\n",
       "      <td>2.679</td>\n",
       "      <td>1.340</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.1405</td>\n",
       "      <td>0.521185</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.057782</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.1287</td>\n",
       "      <td>0.484468</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.059646</td>\n",
       "      <td>32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.1333</td>\n",
       "      <td>0.505078</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.061510</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.1227</td>\n",
       "      <td>0.519858</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.063374</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.466075</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.065238</td>\n",
       "      <td>35</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.1362</td>\n",
       "      <td>0.482440</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.067102</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.445935</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.1296</td>\n",
       "      <td>0.477577</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.070829</td>\n",
       "      <td>38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.434803</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.072693</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.1329</td>\n",
       "      <td>0.425467</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.074557</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.074557</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1138.9442</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.035</td>\n",
       "      <td>2.781502e+15</td>\n",
       "      <td>0.238119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loss  grad_norm  learning_rate     epoch  step  eval_loss  eval_runtime  \\\n",
       "0   0.4614   1.675442       0.000000  0.001864     1        NaN           NaN   \n",
       "1   0.4720   1.648422       0.000019  0.003728     2        NaN           NaN   \n",
       "2   0.4780   1.745430       0.000037  0.005592     3        NaN           NaN   \n",
       "3   0.4447   1.534928       0.000056  0.007456     4        NaN           NaN   \n",
       "4   0.4412   1.506474       0.000075  0.009320     5        NaN           NaN   \n",
       "5   0.4309   1.384470       0.000094  0.011184     6        NaN           NaN   \n",
       "6   0.4024   1.212762       0.000112  0.013048     7        NaN           NaN   \n",
       "7   0.3757   1.072563       0.000131  0.014911     8        NaN           NaN   \n",
       "8   0.3602   1.059663       0.000150  0.016775     9        NaN           NaN   \n",
       "9   0.3207   1.018710       0.000145  0.018639    10        NaN           NaN   \n",
       "10  0.3042   0.986104       0.000141  0.020503    11        NaN           NaN   \n",
       "11  0.2915   1.168620       0.000136  0.022367    12        NaN           NaN   \n",
       "12  0.2965   1.099148       0.000131  0.024231    13        NaN           NaN   \n",
       "13  0.2722   1.118678       0.000127  0.026095    14        NaN           NaN   \n",
       "14  0.2613   1.096027       0.000122  0.027959    15        NaN           NaN   \n",
       "15     NaN        NaN            NaN  0.027959    15   1.949615      411.4148   \n",
       "16  0.2521   1.228678       0.000117  0.029823    16        NaN           NaN   \n",
       "17  0.2327   1.295628       0.000113  0.031687    17        NaN           NaN   \n",
       "18  0.2291   1.552722       0.000108  0.033551    18        NaN           NaN   \n",
       "19  0.1929   1.489771       0.000103  0.035415    19        NaN           NaN   \n",
       "20  0.1929   1.344504       0.000098  0.037279    20        NaN           NaN   \n",
       "21  0.1726   1.119166       0.000094  0.039143    21        NaN           NaN   \n",
       "22  0.1656   1.016779       0.000089  0.041007    22        NaN           NaN   \n",
       "23  0.1709   0.805474       0.000084  0.042870    23        NaN           NaN   \n",
       "24  0.1600   0.904257       0.000080  0.044734    24        NaN           NaN   \n",
       "25  0.1621   0.690987       0.000075  0.046598    25        NaN           NaN   \n",
       "26  0.1504   0.627642       0.000070  0.048462    26        NaN           NaN   \n",
       "27  0.1526   1.395633       0.000066  0.050326    27        NaN           NaN   \n",
       "28  0.1404   0.523600       0.000061  0.052190    28        NaN           NaN   \n",
       "29  0.1198   0.568994       0.000056  0.054054    29        NaN           NaN   \n",
       "30  0.1383   0.547417       0.000052  0.055918    30        NaN           NaN   \n",
       "31     NaN        NaN            NaN  0.055918    30   1.046528      328.4405   \n",
       "32  0.1405   0.521185       0.000047  0.057782    31        NaN           NaN   \n",
       "33  0.1287   0.484468       0.000042  0.059646    32        NaN           NaN   \n",
       "34  0.1333   0.505078       0.000038  0.061510    33        NaN           NaN   \n",
       "35  0.1227   0.519858       0.000033  0.063374    34        NaN           NaN   \n",
       "36  0.1155   0.466075       0.000028  0.065238    35        NaN           NaN   \n",
       "37  0.1362   0.482440       0.000024  0.067102    36        NaN           NaN   \n",
       "38  0.1201   0.445935       0.000019  0.068966    37        NaN           NaN   \n",
       "39  0.1296   0.477577       0.000014  0.070829    38        NaN           NaN   \n",
       "40  0.1200   0.434803       0.000009  0.072693    39        NaN           NaN   \n",
       "41  0.1329   0.425467       0.000005  0.074557    40        NaN           NaN   \n",
       "42     NaN        NaN            NaN  0.074557    40        NaN           NaN   \n",
       "\n",
       "    eval_samples_per_second  eval_steps_per_second  train_runtime  \\\n",
       "0                       NaN                    NaN            NaN   \n",
       "1                       NaN                    NaN            NaN   \n",
       "2                       NaN                    NaN            NaN   \n",
       "3                       NaN                    NaN            NaN   \n",
       "4                       NaN                    NaN            NaN   \n",
       "5                       NaN                    NaN            NaN   \n",
       "6                       NaN                    NaN            NaN   \n",
       "7                       NaN                    NaN            NaN   \n",
       "8                       NaN                    NaN            NaN   \n",
       "9                       NaN                    NaN            NaN   \n",
       "10                      NaN                    NaN            NaN   \n",
       "11                      NaN                    NaN            NaN   \n",
       "12                      NaN                    NaN            NaN   \n",
       "13                      NaN                    NaN            NaN   \n",
       "14                      NaN                    NaN            NaN   \n",
       "15                    2.139                  1.069            NaN   \n",
       "16                      NaN                    NaN            NaN   \n",
       "17                      NaN                    NaN            NaN   \n",
       "18                      NaN                    NaN            NaN   \n",
       "19                      NaN                    NaN            NaN   \n",
       "20                      NaN                    NaN            NaN   \n",
       "21                      NaN                    NaN            NaN   \n",
       "22                      NaN                    NaN            NaN   \n",
       "23                      NaN                    NaN            NaN   \n",
       "24                      NaN                    NaN            NaN   \n",
       "25                      NaN                    NaN            NaN   \n",
       "26                      NaN                    NaN            NaN   \n",
       "27                      NaN                    NaN            NaN   \n",
       "28                      NaN                    NaN            NaN   \n",
       "29                      NaN                    NaN            NaN   \n",
       "30                      NaN                    NaN            NaN   \n",
       "31                    2.679                  1.340            NaN   \n",
       "32                      NaN                    NaN            NaN   \n",
       "33                      NaN                    NaN            NaN   \n",
       "34                      NaN                    NaN            NaN   \n",
       "35                      NaN                    NaN            NaN   \n",
       "36                      NaN                    NaN            NaN   \n",
       "37                      NaN                    NaN            NaN   \n",
       "38                      NaN                    NaN            NaN   \n",
       "39                      NaN                    NaN            NaN   \n",
       "40                      NaN                    NaN            NaN   \n",
       "41                      NaN                    NaN            NaN   \n",
       "42                      NaN                    NaN      1138.9442   \n",
       "\n",
       "    train_samples_per_second  train_steps_per_second    total_flos  train_loss  \n",
       "0                        NaN                     NaN           NaN         NaN  \n",
       "1                        NaN                     NaN           NaN         NaN  \n",
       "2                        NaN                     NaN           NaN         NaN  \n",
       "3                        NaN                     NaN           NaN         NaN  \n",
       "4                        NaN                     NaN           NaN         NaN  \n",
       "5                        NaN                     NaN           NaN         NaN  \n",
       "6                        NaN                     NaN           NaN         NaN  \n",
       "7                        NaN                     NaN           NaN         NaN  \n",
       "8                        NaN                     NaN           NaN         NaN  \n",
       "9                        NaN                     NaN           NaN         NaN  \n",
       "10                       NaN                     NaN           NaN         NaN  \n",
       "11                       NaN                     NaN           NaN         NaN  \n",
       "12                       NaN                     NaN           NaN         NaN  \n",
       "13                       NaN                     NaN           NaN         NaN  \n",
       "14                       NaN                     NaN           NaN         NaN  \n",
       "15                       NaN                     NaN           NaN         NaN  \n",
       "16                       NaN                     NaN           NaN         NaN  \n",
       "17                       NaN                     NaN           NaN         NaN  \n",
       "18                       NaN                     NaN           NaN         NaN  \n",
       "19                       NaN                     NaN           NaN         NaN  \n",
       "20                       NaN                     NaN           NaN         NaN  \n",
       "21                       NaN                     NaN           NaN         NaN  \n",
       "22                       NaN                     NaN           NaN         NaN  \n",
       "23                       NaN                     NaN           NaN         NaN  \n",
       "24                       NaN                     NaN           NaN         NaN  \n",
       "25                       NaN                     NaN           NaN         NaN  \n",
       "26                       NaN                     NaN           NaN         NaN  \n",
       "27                       NaN                     NaN           NaN         NaN  \n",
       "28                       NaN                     NaN           NaN         NaN  \n",
       "29                       NaN                     NaN           NaN         NaN  \n",
       "30                       NaN                     NaN           NaN         NaN  \n",
       "31                       NaN                     NaN           NaN         NaN  \n",
       "32                       NaN                     NaN           NaN         NaN  \n",
       "33                       NaN                     NaN           NaN         NaN  \n",
       "34                       NaN                     NaN           NaN         NaN  \n",
       "35                       NaN                     NaN           NaN         NaN  \n",
       "36                       NaN                     NaN           NaN         NaN  \n",
       "37                       NaN                     NaN           NaN         NaN  \n",
       "38                       NaN                     NaN           NaN         NaN  \n",
       "39                       NaN                     NaN           NaN         NaN  \n",
       "40                       NaN                     NaN           NaN         NaN  \n",
       "41                       NaN                     NaN           NaN         NaN  \n",
       "42                     0.281                   0.035  2.781502e+15    0.238119  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "df_eval = df[df[\"eval_loss\"].notnull()]                     # ‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "final_eval = df_eval.iloc[-1]                               # ‡∏î‡∏∂‡∏á‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "eval_loss_final = final_eval[\"eval_loss\"]\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(\"Training loss   =\", trainer_stats.training_loss)\n",
    "print(f\"Final Eval Loss = {eval_loss_final:.4f}\")\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea9e0a2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "596588c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model saved to: lora_model_val_final_20251006_012140\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_val_final_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa201501",
   "metadata": {},
   "source": [
    "#### ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Å‡∏±‡∏ö Test Set (Final Evaluation)\n",
    "\n",
    "‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• test (‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏Ñ‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏¢)\n",
    "‡πÉ‡∏´‡πâ‡∏£‡∏±‡∏ô trainer.evaluate(test_dataset) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π performance ‡∏ó‡∏µ‡πà‡πÅ‡∏ó‡πâ‡∏à‡∏£‡∏¥‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "484225ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='457' max='457' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [457/457 05:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9748682379722595, 'eval_runtime': 311.97, 'eval_samples_per_second': 2.927, 'eval_steps_per_second': 1.465, 'epoch': 0.07455731593662628}\n"
     ]
    }
   ],
   "source": [
    "final_eval = trainer.evaluate(converted_dataset_test)\n",
    "print(final_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "197857ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 0, true_class : Degenerative_Infectious, pred_class : Degenerative_Infectious\n",
      "id: 1, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 2, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 3, true_class : Obstructive, pred_class : Higher_Density\n",
      "id: 4, true_class : Higher_Density, pred_class : Higher_Density\n",
      "id: 5, true_class : Obstructive, pred_class : Inflammatory_Pneumonia\n",
      "id: 6, true_class : Inflammatory_Pneumonia, pred_class : Higher_Density\n",
      "id: 7, true_class : Normal, pred_class : Normal\n",
      "id: 8, true_class : Obstructive, pred_class : Higher_Density\n",
      "id: 9, true_class : Degenerative_Infectious, pred_class : Higher_Density\n",
      "id: 10, true_class : Normal, pred_class : Higher_Density\n",
      "id: 11, true_class : Lower_Density, pred_class : Normal\n",
      "id: 12, true_class : Inflammatory_Pneumonia, pred_class : Normal\n",
      "id: 13, true_class : Normal, pred_class : Higher_Density\n",
      "id: 14, true_class : Mediastinal_Changes, pred_class : Higher_Density\n",
      "id: 15, true_class : Obstructive, pred_class : Higher_Density\n",
      "id: 16, true_class : Normal, pred_class : Higher_Density\n",
      "id: 17, true_class : Mediastinal_Changes, pred_class : Inflammatory_Pneumonia\n",
      "id: 18, true_class : Obstructive, pred_class : Higher_Density\n",
      "id: 19, true_class : Degenerative_Infectious, pred_class : Higher_Density\n",
      "id: 20, true_class : Inflammatory_Pneumonia, pred_class : Higher_Density\n",
      "id: 21, true_class : Lower_Density, pred_class : Higher_Density\n",
      "id: 22, true_class : Inflammatory_Pneumonia, pred_class : Higher_Density\n",
      "id: 23, true_class : Lower_Density, pred_class : Normal\n",
      "id: 24, true_class : Degenerative_Infectious, pred_class : Inflammatory_Pneumonia\n",
      "id: 25, true_class : Lower_Density, pred_class : Higher_Density\n",
      "id: 26, true_class : Inflammatory_Pneumonia, pred_class : Higher_Density\n",
      "id: 27, true_class : Mediastinal_Changes, pred_class : Higher_Density\n",
      "id: 28, true_class : Chest_Changes, pred_class : Higher_Density\n",
      "id: 29, true_class : Normal, pred_class : Inflammatory_Pneumonia\n",
      "\n",
      "‚úÖ Manual eval_accuracy on 30 samples: 0.1000 (3/30)\n",
      "üíæ Saved CSV -> vl_eval_predictions_final.csv\n"
     ]
    }
   ],
   "source": [
    "import re, torch, pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ===== 1) ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡πà 30 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å =====\n",
    "subset = blind_test_hf.select(range(30))\n",
    "\n",
    "CLASS_SET = {\n",
    "    \"Chest_Changes\",\"Degenerative_Infectious\",\"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\",\"Lower_Density\",\"Mediastinal_Changes\",\n",
    "    \"Normal\",\"Obstructive\",\n",
    "}\n",
    "\n",
    "instruction = (\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "    \"Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def extract_class_strict(text: str):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None\n",
    "    # 1) ‡∏à‡∏±‡∏ö‡∏à‡∏≤‡∏Å \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\" ‡∏Å‡πà‡∏≠‡∏ô (multi-line, case-insensitive)\n",
    "    m = re.search(r\"(?im)^\\s*Class\\s*:\\s*([A-Za-z_]+)\", text)\n",
    "    if m:\n",
    "        cand = m.group(1)\n",
    "        # ‡πÑ‡∏°‡πà‡∏™‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å‡πÉ‡∏´‡∏ç‡πà ‡πÅ‡∏ï‡πà‡∏Ñ‡∏∑‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô\n",
    "        for cls in CLASS_SET:\n",
    "            if cand.lower() == cls.lower():\n",
    "                return cls\n",
    "        return None\n",
    "    # 2) Fallback: ‡∏ñ‡πâ‡∏≤‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏Ñ‡πà‡∏≠‡∏¢‡∏™‡πÅ‡∏Å‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á prompt echo)\n",
    "    for cls in sorted(CLASS_SET, key=str.lower):\n",
    "        if re.search(rf\"\\b{re.escape(cls)}\\b\", text, flags=re.IGNORECASE):\n",
    "            return cls\n",
    "    return None\n",
    "\n",
    "rows = []\n",
    "correct = total = 0\n",
    "\n",
    "for i, example in enumerate(subset):\n",
    "    image = example[\"image\"]\n",
    "\n",
    "    # label ‡∏à‡∏£‡∏¥‡∏á‡∏°‡∏≤‡∏à‡∏≤‡∏Å __class__\n",
    "    true_class = example[\"__class__\"]\n",
    "\n",
    "    # image_id (‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå id ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏ä‡∏ô‡∏µ i)\n",
    "    image_id = example.get(\"id\", i)\n",
    "\n",
    "    # [changed] ‡∏™‡πà‡∏á instruction ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô‡∏ó‡∏∏‡∏Å‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤ chat template\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": instruction},\n",
    "        ]}\n",
    "    ]\n",
    "    input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=128,\n",
    "            do_sample=False,\n",
    "            use_cache=True,\n",
    "            return_dict_in_generate=False,\n",
    "            output_scores=False,\n",
    "        )\n",
    "\n",
    "    # ‡∏ï‡∏±‡∏î prompt ‡∏≠‡∏≠‡∏Å: ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß input_ids ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡πà‡∏á\n",
    "    input_len = inputs[\"input_ids\"].shape[1]\n",
    "    gen_ids = out_ids[0][input_len:]                 # <-- ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà\n",
    "    decoded_text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
    "    \n",
    "    pred_class = extract_class_strict(decoded_text)\n",
    "    print(f\"id: {image_id}, true_class : {true_class}, pred_class : {pred_class}\")\n",
    "\n",
    "    # ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï accuracy (‡∏ô‡∏±‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ true_class ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™‡πÉ‡∏ô ALLOWED_CLASSES)\n",
    "    if true_class in CLASS_SET:\n",
    "        total += 1\n",
    "        if pred_class == true_class:\n",
    "            correct += 1\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": image_id,\n",
    "        \"pred_class\": pred_class,\n",
    "        \"true_class\": true_class,\n",
    "        \"decoded_text\": decoded_text,\n",
    "    })\n",
    "\n",
    "# ===== 2) ‡∏™‡∏£‡∏∏‡∏õ accuracy =====\n",
    "acc = correct / total if total > 0 else 0.0\n",
    "print(f\"\\n‚úÖ Manual eval_accuracy on 30 samples: {acc:.4f} ({correct}/{total})\")\n",
    "\n",
    "# ===== 3) ‡πÄ‡∏ã‡∏ü CSV =====\n",
    "df = pd.DataFrame(rows, columns=[\"image_id\", \"pred_class\", \"true_class\", \"decoded_text\"])\n",
    "csv_path = f\"vl_eval_predictions_final.csv\"\n",
    "df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"üíæ Saved CSV -> {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
