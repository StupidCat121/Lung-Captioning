{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6569db01",
   "metadata": {},
   "source": [
    "# Train ‡πÅ‡∏•‡∏∞ Evaluate ‡πÇ‡∏°‡πÄ‡∏î‡∏•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314abfd3",
   "metadata": {},
   "source": [
    "### 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞ Import Library\n",
    "\n",
    "- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏ä‡πà‡∏ô unsloth, transformers, trl\n",
    "- transformers ‡∏Ñ‡∏∑‡∏≠ library ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á HuggingFace ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• NLP ‡πÅ‡∏•‡∏∞ Vision ‡∏ó‡∏µ‡πà pretrained ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß\n",
    "- trl ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ train/fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ reinforcement learning ‡∏´‡∏£‡∏∑‡∏≠ supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41bd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤ env\n",
    "# python -m venv .venv\n",
    "# # Windows: .\\.venv\\Scripts\\Activate.ps1\n",
    "# # Linux/macOS: source .venv/bin/activate\n",
    "# python -m pip install -U pip\n",
    "\n",
    "# # 2) ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤\n",
    "# pip uninstall -y torch torchvision torchaudio xformers\n",
    "\n",
    "# # 3) ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch + CUDA runtime (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏ó‡∏≤‡∏á)\n",
    "# pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
    "# # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ Get Started ‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô CUDA ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "\n",
    "# # 4) xformers (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô)\n",
    "# pip install xformers -f https://download.pytorch.org/whl/xformers/\n",
    "\n",
    "# # 5) Unsloth\n",
    "# pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954f373",
   "metadata": {},
   "source": [
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ PyTorch ‡πÄ‡∏´‡πá‡∏ô GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fe0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Built with CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2fc6a",
   "metadata": {},
   "source": [
    "### 2. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Vision-Language\n",
    "\n",
    "- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Qwen2.5-VL (Vision-Language model) ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û + ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "- tokenizer ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (tokens)\n",
    "- load_in_4bit=True ‚Üí ‡πÉ‡∏ä‡πâ quantization 4-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ca3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1004 23:35:55.926000 18320 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\namth\\anaconda3\\envs\\lung_lora\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:35.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"{DEVICE_TYPE}:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.11: Fast Qwen2_5_Vl patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4050 Laptop GPU. Num GPUs = 1. Max memory: 5.997 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.9. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/Qwen2.5-VL-3B-Instruct-bnb-4bit\",\n",
    "    load_in_4bit = True,\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74992d2e",
   "metadata": {},
   "source": [
    "### 3. ‡πÄ‡∏û‡∏¥‡πà‡∏° LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ LoRA (Low-Rank Adaptation) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ train ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "- ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ù‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ vision part ‡∏´‡∏£‡∏∑‡∏≠ language part ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ccccafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True, # False if not finetuning vision layers\n",
    "    finetune_language_layers   = True, # False if not finetuning language layers\n",
    "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
    "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
    "\n",
    "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha = 16,  # Recommended alpha == r at least\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8401401c",
   "metadata": {},
   "source": [
    "### 4. ‡πÇ‡∏´‡∏•‡∏î Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d104395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 6085\n",
      "})\n",
      "{'image': Image(mode=None, decode=True), 'text': Value('string'), '__class__': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "print(hf)\n",
    "print(hf.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ee72f",
   "metadata": {},
   "source": [
    "- ‡∏™‡πà‡∏á train_hf ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô train_dataset ‡πÅ‡∏•‡∏∞ val_hf ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô eval_dataset ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ù‡∏∂‡∏Å\n",
    "- ‡∏ï‡∏≠‡∏ô‡∏à‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏Å trainer.evaluate(eval_dataset=test_hf) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ú‡∏• ‡∏ö‡∏ô test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "802ccea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 4259\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 913\n",
      "})\n",
      "Dataset({\n",
      "    features: ['image', 'text', '__class__'],\n",
      "    num_rows: 913\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ‡πÅ‡∏ö‡πà‡∏á train/test (30%) ‡∏Å‡πà‡∏≠‡∏ô\n",
    "splits = hf.train_test_split(test_size=0.3, seed=42)\n",
    "train_hf = splits[\"train\"]\n",
    "tmp_hf   = splits[\"test\"]\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á tmp ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô val/test ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡∏Ñ‡∏£‡∏∂‡πà‡∏á ‚Üí ‡πÑ‡∏î‡πâ 15/15\n",
    "vt = tmp_hf.train_test_split(test_size=0.5, seed=42)\n",
    "val_hf  = vt[\"train\"]\n",
    "test_hf = vt[\"test\"]\n",
    "\n",
    "print(train_hf)\n",
    "print(val_hf)\n",
    "print(test_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b13f9c",
   "metadata": {},
   "source": [
    "### 5. ‡πÅ‡∏õ‡∏•‡∏á Dataset ‡πÄ‡∏õ‡πá‡∏ô Conversation Format\n",
    "\n",
    "- ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Chat (user ‚Üí assistant)\n",
    "- user ‚Üí ‡∏™‡πà‡∏á‡∏†‡∏≤‡∏û + ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á\n",
    "- assistant ‚Üí ‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏î‡πâ‡∏ß‡∏¢ LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.\"\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    cls_name = sample[\"__class__\"]\n",
    "    description = sample[\"text\"]\n",
    "\n",
    "    answer = f\"Class: {cls_name}\\nExplanation: {description}\"\n",
    "    \n",
    "    conversation = [\n",
    "        {\"role\": \"user\", \"content\" : [\n",
    "            {\"type\" : \"text\",  \"text\"  : instruction},\n",
    "            {\"type\" : \"image\", \"image\" : sample[\"image\"]} ]\n",
    "        },\n",
    "        {\"role\" : \"assistant\", \"content\" : [\n",
    "            {\"type\" : \"text\", \"text\" : answer} ]\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\" : conversation}\n",
    "\n",
    "converted_dataset = [convert_to_conversation(sample) for sample in train_hf]\n",
    "\n",
    "converted_dataset_val = [convert_to_conversation(sample) for sample in val_hf]\n",
    "\n",
    "converted_dataset_test = [convert_to_conversation(sample) for sample in test_hf]\n",
    "\n",
    "converted_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f70879",
   "metadata": {},
   "source": [
    "### 6. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏ù‡∏∂‡∏Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17148a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = hf[0][\"image\"]\n",
    "instruction = \"Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                    use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f20275e",
   "metadata": {},
   "source": [
    "### 7. ‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Training)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ SFTTrainer ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£ supervised fine-tuning\n",
    "- ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• converted_dataset ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏π‡πà (‡∏†‡∏≤‡∏û ‚Üí LaTeX)\n",
    "- ‡∏Å‡∏≥‡∏´‡∏ô‡∏î hyperparameters ‡πÄ‡∏ä‡πà‡∏ô batch size, learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecadcd",
   "metadata": {},
   "source": [
    "‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏™‡∏î‡∏á Performance Graph ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6fd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [lbl.strip() for lbl in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"].mid.fmeasure,\n",
    "        \"rougeL\": result[\"rougeL\"].mid.fmeasure,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e31448",
   "metadata": {},
   "source": [
    "‡πÅ‡∏•‡πâ‡∏ß‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏ô SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23dff3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "FastVisionModel.for_training(model) # Enable for training!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer), # Must use!\n",
    "    train_dataset = converted_dataset,\n",
    "    eval_dataset=converted_dataset_val,   # ‡πÄ‡∏û‡∏¥‡πà‡∏° validation set\n",
    "    args = SFTConfig(\n",
    "        # ===== Training schedule =====\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 30,\n",
    "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
    "        seed = 3407,\n",
    "        \n",
    "        # ===== Eval =====\n",
    "        eval_strategy = \"steps\",         # ‡∏´‡∏£‡∏∑‡∏≠ \"epoch\"\n",
    "        eval_steps = 5,                  # ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ \"steps\"\n",
    "        prediction_loss_only = True,     # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: ‡πÑ‡∏°‡πà‡∏î‡∏∂‡∏á logits/preds\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "\n",
    "        # ===== Precision / dtype =====\n",
    "        fp16 = not is_bf16_supported(),\n",
    "        bf16 = is_bf16_supported(),\n",
    "\n",
    "        # ===== Optimization =====\n",
    "        learning_rate = 2e-4,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "\n",
    "        # ===== Logging / reporting =====\n",
    "        report_to = \"none\",\n",
    "        logging_strategy = \"steps\",\n",
    "        logging_steps = 1,\n",
    "\n",
    "        # ===== Output =====\n",
    "        output_dir = \"outputs\",\n",
    "\n",
    "        # ===== Vision finetuning (required) =====\n",
    "        # You MUST put the below items for vision finetuning:\n",
    "        remove_unused_columns = False,\n",
    "        dataset_text_field = \"\",\n",
    "        dataset_kwargs = {\"skip_prepare_dataset\": True},\n",
    "        dataset_num_proc = 4,\n",
    "        max_seq_length = 2048,\n",
    "    ),\n",
    "    compute_metrics=compute_metrics  # ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡∏°‡∏≤‡∏î\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc8b022",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d31789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_stats = trainer.train()\n",
    "\n",
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa7a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c51c7",
   "metadata": {},
   "source": [
    "#### ‡πÅ‡∏™‡∏î‡∏á Performance Graph ‡πÄ‡∏ä‡πà‡∏ô Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be935f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_values = trainer_stats.training_loss  # ‡∏´‡∏£‡∏∑‡∏≠ trainer.state.log_history\n",
    "\n",
    "loss_list = [x[\"loss\"] for x in trainer.state.log_history if \"loss\" in x]\n",
    "\n",
    "plt.plot(loss_list, label=\"Training Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(len(loss_list))\n",
    "print(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd732562",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = trainer.state.log_history\n",
    "\n",
    "# ‡∏î‡∏∂‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ eval_loss\n",
    "eval_points = [h for h in history if \"eval_loss\" in h]\n",
    "\n",
    "steps = [h.get(\"step\", i) for i, h in enumerate(eval_points)]\n",
    "eval_losses = [h[\"eval_loss\"] for h in eval_points]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(steps, eval_losses, marker=\"o\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"eval_loss\")\n",
    "plt.title(\"Validation Loss over time\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "train_points = [h for h in history if \"loss\" in h and \"eval_loss\" not in h]\n",
    "train_steps = [h.get(\"step\", i) for i, h in enumerate(train_points)]\n",
    "train_losses = [h[\"loss\"] for h in train_points]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_steps, train_losses, label=\"train loss\")\n",
    "plt.plot(steps, eval_losses, label=\"eval loss\")\n",
    "plt.xlabel(\"Step\"); plt.ylabel(\"Loss\"); plt.grid(True); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(history)\n",
    "print(df.shape)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af9d5a1",
   "metadata": {},
   "source": [
    "### 8. ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏á‡∏ù‡∏∂‡∏Å\n",
    "\n",
    "‡∏£‡∏±‡∏ô inference ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö performance ‡∏´‡∏•‡∏±‡∏á fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = hf[0][\"image\"]\n",
    "instruction = \"Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                    use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4da832",
   "metadata": {},
   "source": [
    "### 9. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "\n",
    "- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ LoRA adapters ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà full model\n",
    "- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")  # ‡πÄ‡∏ä‡πà‡∏ô 20251004_1658\n",
    "save_dir = f\"lora_model_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d525bf",
   "metadata": {},
   "source": [
    "‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏≠‡∏∞‡πÅ‡∏î‡∏õ‡πÄ‡∏ï‡∏≠‡∏£‡πå LoRA ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏≤‡∏ô ‡πÉ‡∏´‡πâ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ False ‡∏ñ‡∏∂‡∏á True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd6718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from unsloth import FastVisionModel\n",
    "    model, tokenizer = FastVisionModel.from_pretrained(\n",
    "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = True, # Set to False for 16bit LoRA\n",
    "    )\n",
    "    FastVisionModel.for_inference(model) # Enable for inference!\n",
    "\n",
    "image = dataset[0][\"image\"]\n",
    "instruction = \"Write the LaTeX representation for this image.\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "        {\"type\": \"image\"},\n",
    "        {\"type\": \"text\", \"text\": instruction}\n",
    "    ]}\n",
    "]\n",
    "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt = True)\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens = False,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                    use_cache = True, temperature = 1.5, min_p = 0.1)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lung_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
