{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "262743a4",
   "metadata": {},
   "source": [
    "Cell 1: Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687e0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/gemma-3-4b-it\")\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
    "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(processor.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b5e60",
   "metadata": {},
   "source": [
    "Cell 2: Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ghostbat101/lung-disease-clinical-texts-and-image-processed\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1209352b",
   "metadata": {},
   "source": [
    "Cell 3: Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a66969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"Dataset root:\", path)\n",
    "print(\"Subfolders:\", os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee73024",
   "metadata": {},
   "source": [
    "Cell 4: Dataloading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0674b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "clinical_file = os.path.join(path, \"lung_disease_clinical_texts_processed.csv\")\n",
    "image_file = os.path.join(path, \"lung_disease_images_processed.csv\")\n",
    "\n",
    "# Read clinical text\n",
    "clinical_data = []\n",
    "with open(clinical_file, newline='', encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        clinical_data.append(row)\n",
    "\n",
    "print(\"Clinical columns:\", clinical_data[0].keys())\n",
    "print(\"First row (clinical):\", clinical_data[0])\n",
    "\n",
    "# Read image data\n",
    "image_data = []\n",
    "with open(image_file, newline='', encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        image_data.append(row)\n",
    "\n",
    "print(\"Image columns:\", image_data[0].keys())\n",
    "print(\"First row (image):\", image_data[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d830e9",
   "metadata": {},
   "source": [
    "Cell 5: Dataset Processing and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc0ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataset = []\n",
    "for clinical_row, image_row in tqdm(zip(clinical_data, image_data), total=len(clinical_data)):\n",
    "    # Put <image> token at the start of every instruction\n",
    "    instruction_text = \"<image> Describe the findings in this lung X-ray.\"\n",
    "\n",
    "    dataset.append({\n",
    "        \"instruction\": instruction_text,\n",
    "        \"input\": [float(image_row[f\"Pixel_{i}\"]) for i in range(65536)],\n",
    "        \"output\": clinical_row.get(\"clinical_text\", \"\").strip(),\n",
    "        \"disease\": clinical_row.get(\"disease\", \"\")\n",
    "    })\n",
    "\n",
    "# Save as a single pickled object (simple and consistent)\n",
    "with open('processed_dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n",
    "print(\"Saved processed dataset with <image> inserted in instruction. Total:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912ab9d",
   "metadata": {},
   "source": [
    "Cell 6: Data Streaming Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"processed_dataset.pkl\", \"rb\") as f_in, open(\"processed_dataset_stream.pkl\", \"wb\") as f_out:\n",
    "    dataset = pickle.load(f_in)\n",
    "    for item in dataset:\n",
    "        pickle.dump(item, f_out)\n",
    "\n",
    "def stream_dataset(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        while True:\n",
    "            try:\n",
    "                yield pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "\n",
    "count = 0\n",
    "for item in stream_dataset(\"processed_dataset_stream.pkl\"):\n",
    "    count += 1\n",
    "    # process item here (or break early for testing)\n",
    "    # print(item)  # Uncomment to see items\n",
    "\n",
    "print(\"Loaded dataset length:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407708f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example Dataset:\", dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661958fb",
   "metadata": {},
   "source": [
    "Cell 7: Enviroment and Import Setup (Start here if all dataset is loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd88ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "\n",
    "# Then your imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    AutoModelForImageTextToText,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0e693",
   "metadata": {},
   "source": [
    "Cell 8: Model and Processor Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddebe5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fc7e2d9bff4b33b16bb816f24def23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"google/gemma-3-4b-it\"  # \"google/medgemma-4b-it\" \"google/gemma-3-4b-it\"\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    # device_map=\"auto\",\n",
    "    # max_memory={0: \"6GiB\", \"cpu\": \"30GiB\"},  \n",
    "    # trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61d77c4",
   "metadata": {},
   "source": [
    "Cell 9: NOT IN USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d4b5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],  # common in attention layers\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"SEQ_2_SEQ_LM\"\n",
    "# )\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c05e28",
   "metadata": {},
   "source": [
    "Cell 10: Special Token Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8491512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens now: ['<image>']\n"
     ]
    }
   ],
   "source": [
    "special_tokens_dict = {\"additional_special_tokens\": [\"<image>\"]}\n",
    "num_added = processor.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "if num_added > 0:\n",
    "    model.resize_token_embeddings(len(processor.tokenizer))\n",
    "print(\"Special tokens now:\", processor.tokenizer.additional_special_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b6a805",
   "metadata": {},
   "source": [
    "Cell 11-12: Token Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0df8bd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens now: ['<image>']\n",
      "['<image>', '▁Describe', '▁the', '▁findings', '▁in', '▁this', '▁lung', '▁X', '-', 'ray', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Special tokens now:\", processor.tokenizer.additional_special_tokens)\n",
    "print(processor.tokenizer.tokenize(\"<image> Describe the findings in this lung X-ray.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79666f58",
   "metadata": {},
   "source": [
    "Cell 13: Advanced Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8a09e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import tempfile, os\n",
    "# from tqdm import tqdm\n",
    "# import time\n",
    "\n",
    "# def preprocess_dataset_chat_template(\n",
    "#     raw_data,\n",
    "#     processor,\n",
    "#     max_length = 512,\n",
    "#     dir = \"./gemma_preproc\",\n",
    "#     max_items = 8000\n",
    "# ):\n",
    "#     os.makedirs(dir, exist_ok=True)\n",
    "#     processed = []\n",
    "\n",
    "#     n = len(raw_data) if max_items is None else min(max_items, len(raw_data))\n",
    "\n",
    "#     for i, item in enumerate(tqdm(raw_data[:n], desc=\"Processing\")):\n",
    "#         t0 = time.time()\n",
    "#         # rebuild PIL image\n",
    "#         arr = np.array(item[\"input\"], dtype=np.uint8)\n",
    "#         t1 = time.time()\n",
    "#         if arr.size == int(np.sqrt(arr.size))**2:\n",
    "#             side = int(np.sqrt(arr.size))\n",
    "#             pil_img = Image.fromarray(arr.reshape(side, side)).convert(\"RGB\")\n",
    "#             pil_img = pil_img.resize((224, 224))\n",
    "#         else:\n",
    "#             assert arr.size % 3 == 0\n",
    "#             side = int(np.sqrt(arr.size // 3))\n",
    "#             pil_img = Image.fromarray(arr.reshape(side, side, 3)).convert(\"RGB\")\n",
    "#         t2 = time.time()\n",
    "\n",
    "#         # save the image locally\n",
    "#         local_path = os.path.join(dir, f\"img_{i}.png\")\n",
    "#         pil_img.save(local_path)\n",
    "#         t3 = time.time()\n",
    "\n",
    "#         # chat template with image\n",
    "#         messages = [\n",
    "#             {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "#             {\"role\": \"user\", \"content\": [\n",
    "#                 {\"type\": \"image\", \"url\": local_path},\n",
    "#                 {\"type\": \"text\", \"text\": item[\"instruction\"].strip()}\n",
    "#             ]}\n",
    "#         ]\n",
    "#         t4 = time.time()\n",
    "\n",
    "#         inputs = processor.apply_chat_template(\n",
    "#             messages,\n",
    "#             add_generation_prompt = True,\n",
    "#             tokenize = True,\n",
    "#             return_dict = True,\n",
    "#             return_tensors = \"pt\",\n",
    "#             max_length = max_length,  \n",
    "#             padding = \"max_length\",\n",
    "#             truncation = True,\n",
    "#         )\n",
    "#         t5 = time.time()\n",
    "\n",
    "#         # labels (target caption / answer)\n",
    "#         target = item.get(\"output\", item.get(\"disease\", \"\"))\n",
    "#         label_ids = processor.tokenizer(\n",
    "#             target,\n",
    "#             padding = \"max_length\",\n",
    "#             truncation = True,\n",
    "#             max_length = max_length,\n",
    "#             return_tensors = \"pt\"\n",
    "#         ).input_ids\n",
    "#         t6 = time.time()\n",
    "\n",
    "#         processed.append({\n",
    "#             \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "#             \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "#             \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "#             \"labels\": label_ids.squeeze(0),\n",
    "#         })\n",
    "#         t7 = time.time()\n",
    "\n",
    "#         # print(\n",
    "#         #     f\"[{i}] np->arr: {t1-t0:.3f}s | arr->img: {t2-t1:.3f}s | save: {t3-t2:.3f}s | msg: {t4-t3:.3f}s | chat_template: {t5-t4:.3f}s | label: {t6-t5:.3f}s | append: {t7-t6:.3f}s | total: {t7-t0:.3f}s\"\n",
    "#         # )\n",
    "\n",
    "#     return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a334e4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<image>']\n"
     ]
    }
   ],
   "source": [
    "print(processor.tokenizer.additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5a26d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import json\n",
    "\n",
    "# with open(\"processed_dataset.pkl\", \"rb\") as f:\n",
    "#     data = pickle.load(f)\n",
    "\n",
    "# # Optionally, only save the first N items for a quick look\n",
    "# data = data[:1]\n",
    "\n",
    "# with open(\"processed_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"Saved as processed_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7795dc89",
   "metadata": {},
   "source": [
    "Cell 14: Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "664c11e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace your XrayDataset class with this streaming version\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import tempfile, os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "class StreamingXrayDataset(Dataset):\n",
    "    def __init__(self, raw_data, processor, max_length=512, cache_dir=\"./xray_cache\"):\n",
    "        self.raw_data = raw_data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        t0 = time.time()\n",
    "        item = self.raw_data[idx]\n",
    "        arr = np.array(item[\"input\"], dtype=np.uint8)\n",
    "        if arr.size == int(np.sqrt(arr.size))**2:\n",
    "            side = int(np.sqrt(arr.size))\n",
    "            pil_img = Image.fromarray(arr.reshape(side, side)).convert(\"RGB\").resize((224, 224))\n",
    "        else:\n",
    "            side = int(np.sqrt(arr.size // 3))\n",
    "            pil_img = Image.fromarray(arr.reshape(side, side, 3)).convert(\"RGB\")\n",
    "        # Pass pil_img directly to processor\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": pil_img},  # pass PIL image directly\n",
    "                {\"type\": \"text\", \"text\": item[\"instruction\"].strip()}\n",
    "            ]}\n",
    "        ]\n",
    "        inputs = self.processor.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=False,\n",
    "        )\n",
    "        t1 = time.time()\n",
    "        print(f\"[{idx}] chat_template: {t1-t0:.3f}s\")\n",
    "\n",
    "        # Process labels\n",
    "        \n",
    "        target = item.get(\"output\", item.get(\"disease\", \"\"))\n",
    "        label_ids = self.processor.tokenizer(\n",
    "            target,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "        t2 = time.time()\n",
    "        print(f\"[{idx}] label_tokenize: {t2-t1:.3f}s\")\n",
    "        # Clean up temp file\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"pixel_values\": inputs[\"pixel_values\"].squeeze(0),\n",
    "            \"labels\": label_ids.squeeze(0),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "707e4ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # def __getitem__(self, idx):\n",
    "    #     # Process data on-demand instead of pre-processing everything\n",
    "    #     item = self.raw_data[idx]\n",
    "        \n",
    "    #     # Create image\n",
    "    #     arr = np.array(item[\"input\"], dtype=np.uint8)\n",
    "    #     if arr.size == int(np.sqrt(arr.size))**2:\n",
    "    #         side = int(np.sqrt(arr.size))\n",
    "    #         pil_img = Image.fromarray(arr.reshape(side, side)).convert(\"RGB\")\n",
    "    #         pil_img = pil_img.resize((224, 224))\n",
    "    #     else:\n",
    "    #         side = int(np.sqrt(arr.size // 3))\n",
    "    #         pil_img = Image.fromarray(arr.reshape(side, side, 3)).convert(\"RGB\")\n",
    "        \n",
    "    #     # Save temporarily (you can optimize this further by caching)\n",
    "    #     temp_path = os.path.join(self.cache_dir, f\"temp_{idx}.png\")\n",
    "    #     pil_img.save(temp_path)\n",
    "        \n",
    "    #     # Process with chat template\n",
    "    #     messages = [\n",
    "    #         {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "    #         {\"role\": \"user\", \"content\": [\n",
    "    #             {\"type\": \"image\", \"url\": temp_path},\n",
    "    #             {\"type\": \"text\", \"text\": item[\"instruction\"].strip()}\n",
    "    #         ]}\n",
    "    #     ]\n",
    "        \n",
    "    #     inputs = self.processor.apply_chat_template(\n",
    "    #         messages,\n",
    "    #         add_generation_prompt=True,\n",
    "    #         tokenize=True,\n",
    "    #         return_dict=True,\n",
    "    #         return_tensors=\"pt\",\n",
    "    #         max_length=self.max_length,\n",
    "    #         padding=\"max_length\",\n",
    "    #         truncation=False,\n",
    "    #     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8918f36",
   "metadata": {},
   "source": [
    "Cell 15: Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f2a5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Load raw dataset\n",
    "# t0 = time.time()\n",
    "# with open(\"processed_dataset.pkl\", \"rb\") as f:\n",
    "#     raw_dataset = pickle.load(f)\n",
    "# t1 = time.time()\n",
    "# print(f\"Loaded raw dataset with {len(raw_dataset)} items in {t1-t0:.3f}s\")\n",
    "\n",
    "# #_______________Debug___________________________________________________\n",
    "# print(\"Raw dataset length:\", len(raw_dataset))\n",
    "# print(\"Special tokens:\", processor.tokenizer.additional_special_tokens)\n",
    "\n",
    "# # Check that <image> token is present in each instruction\n",
    "# # i = 0\n",
    "# # for item in raw_dataset:\n",
    "# #     if \"<image>\" in item[\"instruction\"]:\n",
    "# #         print(f\"✅ {i}. Image token is present in the prompt!\")\n",
    "# #     else:\n",
    "# #         print(f\"❌ {i}. Image token is NOT present in the prompt!\")\n",
    "# #         print(\"❌ Instruction:\", repr(item[\"instruction\"]))\n",
    "# #         break  # Stop after first failure\n",
    "# #     i += 1\n",
    "# #______________________________________________________________________\n",
    "\n",
    "# # Preprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c378ca38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw dataset with 8000 items in 55.432s\n",
      "Train dataset length: 6400\n",
      "Validation dataset length: 1600\n",
      "[0] chat_template: 0.611s\n",
      "[0] label_tokenize: 0.043s\n",
      "Train example: {'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'pixel_values': torch.Size([3, 896, 896]), 'labels': torch.Size([512])}\n",
      "[0] chat_template: 0.072s\n",
      "[0] label_tokenize: 0.031s\n",
      "Validation example: {'input_ids': torch.Size([512]), 'attention_mask': torch.Size([512]), 'pixel_values': torch.Size([3, 896, 896]), 'labels': torch.Size([512])}\n",
      "Created streaming datasets in 11.150s\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Replace your data processing section with this:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load raw dataset\n",
    "t0 = time.time()\n",
    "with open(\"processed_dataset.pkl\", \"rb\") as f:\n",
    "    raw_dataset = pickle.load(f)\n",
    "t1 = time.time()\n",
    "print(f\"Loaded raw dataset with {len(raw_dataset)} items in {t1-t0:.3f}s\")\n",
    "\n",
    "# Split raw data (not processed data)\n",
    "train_raw, val_raw = train_test_split(raw_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create streaming datasets\n",
    "train_dataset = StreamingXrayDataset(train_raw, processor)\n",
    "val_dataset = StreamingXrayDataset(val_raw, processor)\n",
    "\n",
    "print(\"Train dataset length:\", len(train_dataset))\n",
    "print(\"Validation dataset length:\", len(val_dataset))\n",
    "print(\"Train example:\", {k: v.shape for k, v in train_dataset[0].items()})\n",
    "print(\"Validation example:\", {k: v.shape for k, v in val_dataset[0].items()})\n",
    "\n",
    "t2 = time.time()\n",
    "print(f\"Created streaming datasets in {t2-t1:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2b3894f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] chat_template: 0.057s\n",
      "[0] label_tokenize: 0.004s\n",
      "Example Dataset: {'input_ids': tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      2,    105,   2364,    107,   3048,    659,    496,  11045,\n",
      "         16326, 236761,    110, 255999, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144,\n",
      "        262144, 262144, 262144, 262144, 262144, 262144, 262144, 262144, 256000,\n",
      "           108, 262145,  54234,    506,  12396,    528,    672,  17195,   1684,\n",
      "        236772,   1254, 236761,    106,    107,    105,   4368,    107]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'pixel_values': tensor([[[0.8902, 0.8902, 0.8824,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.8902, 0.8902, 0.8824,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.8824, 0.8824, 0.8667,  ..., 0.8275, 0.8353, 0.8353],\n",
      "         ...,\n",
      "         [0.8431, 0.8431, 0.8275,  ..., 0.8510, 0.8667, 0.8667],\n",
      "         [0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8745, 0.8745],\n",
      "         [0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8745, 0.8745]],\n",
      "\n",
      "        [[0.8902, 0.8902, 0.8824,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.8902, 0.8902, 0.8824,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.8824, 0.8824, 0.8667,  ..., 0.8275, 0.8353, 0.8353],\n",
      "         ...,\n",
      "         [0.8431, 0.8431, 0.8275,  ..., 0.8510, 0.8667, 0.8667],\n",
      "         [0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8745, 0.8745],\n",
      "         [0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8745, 0.8745]],\n",
      "\n",
      "        [[0.8902, 0.8902, 0.8824,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.8902, 0.8902, 0.8824,  ..., 0.8353, 0.8353, 0.8353],\n",
      "         [0.8824, 0.8824, 0.8667,  ..., 0.8275, 0.8353, 0.8353],\n",
      "         ...,\n",
      "         [0.8431, 0.8431, 0.8275,  ..., 0.8510, 0.8667, 0.8667],\n",
      "         [0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8745, 0.8745],\n",
      "         [0.8431, 0.8431, 0.8431,  ..., 0.8745, 0.8745, 0.8745]]]), 'labels': tensor([     0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             2,   8574,    756,    494,   9668, 236764,    532,    564,    740,\n",
      "           756,    494, 236761,   3551,  15350, 236764,    532,    564, 236761,\n",
      "           564,  44245,    532,    625,    756,   1401, 236761,    564,    756,\n",
      "           520,  16937,    529,    756,    503,  13922,    531, 236761])}\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Dataset:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd743dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# print(raw_dataset[0])\n",
    "# print(processed_data[0])\n",
    "\n",
    "# def tensor_to_list(obj):\n",
    "#     if hasattr(obj, \"tolist\"):\n",
    "#         return obj.tolist()\n",
    "#     elif isinstance(obj, dict):\n",
    "#         return {k: tensor_to_list(v) for k, v in obj.items()}\n",
    "#     elif isinstance(obj, list):\n",
    "#         return [tensor_to_list(v) for v in obj]\n",
    "#     else:\n",
    "#         return obj\n",
    "\n",
    "# print(raw_dataset[0])\n",
    "# print(processed_data[0])\n",
    "\n",
    "# # Convert all tensors to lists for JSON serialization\n",
    "# processed_data_serializable = [tensor_to_list(item) for item in processed_data]\n",
    "\n",
    "# with open(\"processed_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(processed_data_serializable, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# print(\"Saved as processed_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a49acb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from bitsandbytes.optim import Adam8bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "72a0e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Add more targets\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7b297f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Gemma3ForConditionalGeneration(\n",
      "      (model): Gemma3Model(\n",
      "        (vision_tower): SiglipVisionModel(\n",
      "          (vision_model): SiglipVisionTransformer(\n",
      "            (embeddings): SiglipVisionEmbeddings(\n",
      "              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
      "              (position_embedding): Embedding(4096, 1152)\n",
      "            )\n",
      "            (encoder): SiglipEncoder(\n",
      "              (layers): ModuleList(\n",
      "                (0-26): 27 x SiglipEncoderLayer(\n",
      "                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                  (self_attn): SiglipAttention(\n",
      "                    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                    (v_proj): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=1152, out_features=4, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=4, out_features=1152, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                    (q_proj): lora.Linear(\n",
      "                      (base_layer): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                      (lora_dropout): ModuleDict(\n",
      "                        (default): Dropout(p=0.05, inplace=False)\n",
      "                      )\n",
      "                      (lora_A): ModuleDict(\n",
      "                        (default): Linear(in_features=1152, out_features=4, bias=False)\n",
      "                      )\n",
      "                      (lora_B): ModuleDict(\n",
      "                        (default): Linear(in_features=4, out_features=1152, bias=False)\n",
      "                      )\n",
      "                      (lora_embedding_A): ParameterDict()\n",
      "                      (lora_embedding_B): ParameterDict()\n",
      "                      (lora_magnitude_vector): ModuleDict()\n",
      "                    )\n",
      "                    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
      "                  )\n",
      "                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "                  (mlp): SiglipMLP(\n",
      "                    (activation_fn): PytorchGELUTanh()\n",
      "                    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
      "                    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (multi_modal_projector): Gemma3MultiModalProjector(\n",
      "          (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
      "          (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
      "        )\n",
      "        (language_model): Gemma3TextModel(\n",
      "          (embed_tokens): Gemma3TextScaledWordEmbedding(262146, 2560, padding_idx=0)\n",
      "          (layers): ModuleList(\n",
      "            (0-33): 34 x Gemma3DecoderLayer(\n",
      "              (self_attn): Gemma3Attention(\n",
      "                (q_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=2560, out_features=2048, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=2560, out_features=4, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=4, out_features=2048, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "                (v_proj): lora.Linear(\n",
      "                  (base_layer): Linear(in_features=2560, out_features=1024, bias=False)\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.05, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=2560, out_features=4, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=4, out_features=1024, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                  (lora_magnitude_vector): ModuleDict()\n",
      "                )\n",
      "                (o_proj): Linear(in_features=2048, out_features=2560, bias=False)\n",
      "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
      "              )\n",
      "              (mlp): Gemma3MLP(\n",
      "                (gate_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "                (up_proj): Linear(in_features=2560, out_features=10240, bias=False)\n",
      "                (down_proj): Linear(in_features=10240, out_features=2560, bias=False)\n",
      "                (act_fn): PytorchGELUTanh()\n",
      "              )\n",
      "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "            )\n",
      "          )\n",
      "          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
      "          (rotary_emb): Gemma3RotaryEmbedding()\n",
      "          (rotary_emb_local): Gemma3RotaryEmbedding()\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2560, out_features=262146, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7056623",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    ignore_index=processor.tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "da672947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # # Shift labels for causal LM (important!)\n",
    "        # shift_logits = logits[..., :-1, :].contiguous()\n",
    "        # shift_labels = labels[..., 1:].contiguous()\n",
    "        \n",
    "        # # Flatten for CrossEntropyLoss\n",
    "        # loss_fct = nn.CrossEntropyLoss(ignore_index=processor.tokenizer.pad_token_id)\n",
    "        # loss = loss_fct(\n",
    "        #     shift_logits.view(-1, shift_logits.size(-1)),\n",
    "        #     shift_labels.view(-1)\n",
    "        # )\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cce54a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, fused=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "379d8bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "\n",
    "    # # 🚀 Make training faster\n",
    "    # eval_strategy=\"no\",    # disable eval during training (run separately later)\n",
    "    # save_strategy=\"epoch\",       # save only once per epoch\n",
    "    # save_total_limit=1,          # keep just final checkpoint\n",
    "    # load_best_model_at_end=False,# skip extra eval+loading\n",
    "\n",
    "    # Logging\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,           # less frequent logs\n",
    "\n",
    "    # Performance\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=None,\n",
    ")\n",
    "\n",
    "\n",
    "# 1. TRAINING ARGUMENTS WITH CHECKPOINTING\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./results\",                    # Main checkpoint directory\n",
    "#     per_device_train_batch_size=4,\n",
    "#     per_device_eval_batch_size=4,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=1,\n",
    "#     eval_strategy=\"no\",\n",
    "#     eval_steps=200,\n",
    "    \n",
    "#     # ✅ CHECKPOINTING SETTINGS\n",
    "#     save_strategy=\"epoch\",                     # Save every N steps\n",
    "#     save_steps=200,                           # Save checkpoint every 200 steps\n",
    "#     save_total_limit=2,                       # Keep only 5 most recent checkpoints\n",
    "#     save_on_each_node=True,                   # Save on each distributed node\n",
    "    \n",
    "#     # ✅ RESUMING SETTINGS  \n",
    "#     resume_from_checkpoint=None,              # Set this when resuming (see below)\n",
    "#     load_best_model_at_end=True,             # Load best checkpoint at end\n",
    "#     metric_for_best_model=\"eval_loss\",       # Which metric to track for \"best\"\n",
    "#     greater_is_better=False,                 # Lower eval_loss is better\n",
    "    \n",
    "#     # ✅ LOGGING & MONITORING\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=50,\n",
    "#     logging_strategy=\"steps\",\n",
    "    \n",
    "#     # Other settings\n",
    "#     bf16=True,\n",
    "#     fp16=False,\n",
    "#     optim=\"adamw_torch_fused\",\n",
    "#     dataloader_num_workers=4,\n",
    "#     remove_unused_columns=False,\n",
    "#     report_to=None,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0b47bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_latest_checkpoint(output_dir):\n",
    "#     \"\"\"Find the most recent checkpoint in output_dir\"\"\"\n",
    "#     if not os.path.exists(output_dir):\n",
    "#         return None\n",
    "    \n",
    "#     checkpoints = []\n",
    "#     for item in os.listdir(output_dir):\n",
    "#         if item.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(output_dir, item)):\n",
    "#             try:\n",
    "#                 step_num = int(item.split(\"-\")[1])\n",
    "#                 checkpoints.append((step_num, os.path.join(output_dir, item)))\n",
    "#             except ValueError:\n",
    "#                 continue\n",
    "    \n",
    "#     if not checkpoints:\n",
    "#         return None\n",
    "    \n",
    "#     # Return path to checkpoint with highest step number\n",
    "#     latest_step, latest_path = max(checkpoints, key=lambda x: x[0])\n",
    "#     return latest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcd6d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_emergency_checkpoint(trainer, step_name=\"emergency\"):\n",
    "#     \"\"\"Save checkpoint manually (e.g., if you need to stop training)\"\"\"\n",
    "#     checkpoint_dir = f\"./results/checkpoint-{step_name}\"\n",
    "#     trainer.save_model(checkpoint_dir)\n",
    "#     trainer.save_state()\n",
    "#     print(f\"💾 Emergency checkpoint saved to: {checkpoint_dir}\")\n",
    "#     return checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6669d643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_training_with_resume(output_dir=\"./results\"):\n",
    "#     \"\"\"Setup training arguments with automatic resume detection\"\"\"\n",
    "    \n",
    "#     # Check for existing checkpoints\n",
    "#     latest_checkpoint = find_latest_checkpoint(output_dir)\n",
    "    \n",
    "#     if latest_checkpoint:\n",
    "#         print(f\"✅ Found checkpoint: {latest_checkpoint}\")\n",
    "#         print(\"Training will resume from this checkpoint\")\n",
    "        \n",
    "#         # Load trainer state to see progress\n",
    "#         trainer_state_file = os.path.join(latest_checkpoint, \"trainer_state.json\")\n",
    "#         if os.path.exists(trainer_state_file):\n",
    "#             with open(trainer_state_file, 'r') as f:\n",
    "#                 trainer_state = json.load(f)\n",
    "#                 print(f\"📊 Resuming from epoch {trainer_state.get('epoch', 'unknown')}\")\n",
    "#                 print(f\"📊 Resuming from global step {trainer_state.get('global_step', 'unknown')}\")\n",
    "#                 print(f\"📊 Best eval loss so far: {trainer_state.get('best_metric', 'unknown')}\")\n",
    "        \n",
    "#         resume_from = latest_checkpoint\n",
    "#     else:\n",
    "#         print(\"🆕 No existing checkpoints found. Starting fresh training.\")\n",
    "#         resume_from = None\n",
    "    \n",
    "#     # Update training arguments\n",
    "#     training_args.resume_from_checkpoint = resume_from\n",
    "#     return training_args, resume_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c63c7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args, resume_checkpoint = setup_training_with_resume(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6493283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     # Add your custom metrics here (BLEU, ROUGE, etc.)\n",
    "#     return {\"perplexity\": torch.exp(torch.tensor(predictions.mean())).item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "809754f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Calculate total training steps\n",
    "num_training_steps = len(train_dataset) * training_args.num_train_epochs // (\n",
    "    training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps\n",
    ")\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * num_training_steps),  # 10% warmup\n",
    "    num_training_steps=num_training_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99fde881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = get_peft_model(model, lora_config) \n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "595a4f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NITRO\\AppData\\Local\\Temp\\ipykernel_2984\\1315328475.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    # data_collator=MultimodalDataCollator(processor),\n",
    "    tokenizer=processor.tokenizer,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    # compute_metrics=compute_metrics, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d007c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # Disable KV cache during training\n",
    "# model.gradient_checkpointing_enable()  # Save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bad1e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb  # Optional but recommended\n",
    "\n",
    "# Initialize wandb (optional)\n",
    "# wandb.init(project=\"lung-xray-gemma\", name=\"lora-finetuning\")\n",
    "\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d33037a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import EarlyStoppingCallback\n",
    "\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "30d3bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Running validation before training...\")\n",
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Pre-training eval loss: {eval_results['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d9f35225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory: 17.24GB allocated\n",
      "GPU memory: 17.28GB reserved\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(f\"GPU memory: {torch.cuda.memory_allocated()/1e9:.2f}GB allocated\")\n",
    "print(f\"GPU memory: {torch.cuda.memory_reserved()/1e9:.2f}GB reserved\")\n",
    "\n",
    "# trainer.train()\n",
    "\n",
    "# # Save LoRA adapter and processor\n",
    "# model.save_pretrained(\"./lung_lora\")\n",
    "# processor.save_pretrained(\"./lung_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0a551212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Simple training monitor\n",
    "# -------------------------\n",
    "import logging\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "# 1. Enable detailed logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"transformers.trainer\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# 2. Define progress callback\n",
    "class SimpleProgressCallback(TrainerCallback):\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"🚀 Training starting...\")\n",
    "        return control\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"⚡ Step {state.global_step}: Starting training step...\")\n",
    "        return control\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        print(f\"✅ Step {state.global_step}: Completed\")\n",
    "        return control\n",
    "\n",
    "    def on_epoch_begin(self, args, state, control, **kwargs):\n",
    "        print(f\"🚀 Epoch {state.epoch}: Starting...\")\n",
    "        return control\n",
    "\n",
    "    def on_log(self, args, state, control, **kwargs):\n",
    "        print(f\"📊 Step {state.global_step}: Logging metrics\")\n",
    "        return control\n",
    "\n",
    "# 3. Attach AFTER you create the trainer\n",
    "trainer.add_callback(SimpleProgressCallback())\n",
    "\n",
    "# 4. Test single training step\n",
    "# print(\"🧪 Testing single training step...\")\n",
    "# trainer.args.max_steps = 1          # only run 1 step\n",
    "# trainer.args.logging_steps = 1\n",
    "# trainer.args.save_steps = 1\n",
    "\n",
    "# try:\n",
    "#     trainer.train()\n",
    "#     print(\"✅ Single step completed! Training is working.\")\n",
    "\n",
    "#     # Reset for full training\n",
    "# trainer.args.max_steps = -1\n",
    "# trainer.args.num_train_epochs = 3\n",
    "# trainer.args.logging_steps = 10\n",
    "# trainer.args.save_steps = 200\n",
    "\n",
    "#     print(\"🚀 Starting full training...\")\n",
    "#     trainer.train()\n",
    "\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"⚠️ Training interrupted by user\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ Training failed at: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3799a10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 6,400\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 400\n",
      "  Number of trainable parameters = 1,611,776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Training starting...\n",
      "🚀 Epoch 0: Starting...\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"./lung_lora\")\n",
    "processor.save_pretrained(\"./lung_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c49ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NITRO\\AppData\\Local\\Temp\\ipykernel_30672\\2274478160.py:12: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `DebugTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = DebugTrainer(\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from transformers import Trainer\n",
    "\n",
    "# class DebugTrainer(CustomTrainer):\n",
    "#     def training_step(self, model, inputs):\n",
    "#         start = time.time()\n",
    "#         out = super().training_step(model, inputs)\n",
    "#         print(f\"[Step] One training step took {time.time() - start:.3f}s\")\n",
    "#         return out\n",
    "\n",
    "# # Replace Trainer with DebugTrainer\n",
    "# trainer = DebugTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=processor.tokenizer,\n",
    "#     optimizers=(optimizer, scheduler),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8070f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 6,400\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 400\n",
      "  Number of trainable parameters = 1,611,776\n",
      "wandb: Currently logged in as: jiraphat-sabutr (jiraphat-sabutr-king-mongkut-s-institute-of-technology-l) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "c:\\Users\\NITRO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pydantic\\main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>f:\\Lung\\wandb\\run-20250927_022205-0z0krn6k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jiraphat-sabutr-king-mongkut-s-institute-of-technology-l/huggingface/runs/0z0krn6k' target=\"_blank\">young-tree-16</a></strong> to <a href='https://wandb.ai/jiraphat-sabutr-king-mongkut-s-institute-of-technology-l/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jiraphat-sabutr-king-mongkut-s-institute-of-technology-l/huggingface' target=\"_blank\">https://wandb.ai/jiraphat-sabutr-king-mongkut-s-institute-of-technology-l/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jiraphat-sabutr-king-mongkut-s-institute-of-technology-l/huggingface/runs/0z0krn6k' target=\"_blank\">https://wandb.ai/jiraphat-sabutr-king-mongkut-s-institute-of-technology-l/huggingface/runs/0z0krn6k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try:\n",
    "#     trainer.train(resume_from_checkpoint=None) # resume_checkpoint\n",
    "    \n",
    "#     # Save final model\n",
    "#     final_save_path = \"./lung_xray_gemma_final\"\n",
    "#     trainer.save_model(final_save_path)\n",
    "#     processor.save_pretrained(final_save_path)\n",
    "#     print(f\"✅ Final model saved to: {final_save_path}\")\n",
    "    \n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"\\n⚠️  Training interrupted by user!\")\n",
    "#     emergency_path = save_emergency_checkpoint(trainer, \"interrupted\")\n",
    "#     print(f\"💾 Emergency save completed: {emergency_path}\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"\\n❌ Training failed with error: {e}\")\n",
    "#     emergency_path = save_emergency_checkpoint(trainer, \"error\")\n",
    "#     print(f\"💾 Emergency save completed: {emergency_path}\")\n",
    "#     raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63617aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Helper cells inserted on 2025-09-23T07:06:56.255374Z\n",
    "These helper functions were appended here: metric calculators, plotting helpers, evaluation wrapper, history extractor, and Grad-CAM utilities (one function per cell).\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877586f0",
   "metadata": {},
   "source": [
    "### Helper: imports and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab3205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# imports and device\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score\n",
    "from typing import Dict, Any, Tuple, List\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5eb1c6",
   "metadata": {},
   "source": [
    "### compute_regression_metrics, compute_classification_metrics, compute_text_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aecc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_regression_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Return R2, MAE, MSE.\"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    return {\n",
    "        \"r2\": float(r2_score(y_true, y_pred)),\n",
    "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
    "        \"mse\": float(mean_squared_error(y_true, y_pred)),\n",
    "    }\n",
    "\n",
    "def compute_classification_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Return accuracy. y_pred may be probabilities/logits; this function expects integer labels or will argmax if needed.\"\"\"\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "    y_pred = y_pred.ravel()\n",
    "    return {\"accuracy\": float(accuracy_score(y_true, y_pred))}\n",
    "\n",
    "def compute_text_metrics(references: List[str], predictions: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Simple text metrics: exact match rate and average token overlap (simple proxy).\"\"\"\n",
    "    exact = sum(1 for r,p in zip(references, predictions) if r.strip() == p.strip())\n",
    "    exact_rate = exact / max(1, len(references))\n",
    "    overlaps = []\n",
    "    for r,p in zip(references, predictions):\n",
    "        rs = set(r.split())\n",
    "        ps = set(p.split())\n",
    "        if len(rs) + len(ps) == 0:\n",
    "            overlaps.append(1.0)\n",
    "        else:\n",
    "            overlaps.append(len(rs & ps) / max(1, len(rs | ps)))\n",
    "    return {\"exact_match\": float(exact_rate), \"token_overlap\": float(np.mean(overlaps))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb511fe9",
   "metadata": {},
   "source": [
    "### plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a82875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(history: Dict[str, List[float]], title_prefix: str = \"Training\"):\n",
    "    \"\"\"history is a dict like {'train_loss': [...], 'eval_loss': [...], 'accuracy': [...], 'r2': [...]}\"\"\"\n",
    "    keys = list(history.keys())\n",
    "    n = len(keys)\n",
    "    plt.figure(figsize=(6, 3*n))\n",
    "    for i, k in enumerate(keys):\n",
    "        plt.subplot(n, 1, i+1)\n",
    "        plt.plot(history[k], marker='o')\n",
    "        plt.title(f\"{title_prefix} - {k}\")\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e7156",
   "metadata": {},
   "source": [
    "### evaluate_trainer(trainer, eval_dataset, task, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_trainer(trainer, eval_dataset, task: str = 'regression', label_extractor=None, tokenizer=None, text_label_key='output'):\n",
    "    \"\"\"Run trainer.predict and compute metrics.\n",
    "    - task: 'regression' | 'classification' | 'text'\n",
    "    - label_extractor: optional callable to extract numeric labels from label_ids or dataset entries\n",
    "    - tokenizer: required for text decoding (if task == 'text')\n",
    "    - text_label_key: dataset key that contains reference text (for dataset entries) when task=='text'\n",
    "    Returns: dict(metrics) and raw predictions\n",
    "    \"\"\"\n",
    "    print('Running prediction (this may take some time) ...')\n",
    "    pred_out = trainer.predict(eval_dataset)\n",
    "    preds = pred_out.predictions\n",
    "    label_ids = pred_out.label_ids\n",
    "\n",
    "    # If text generation: predictions will often be token ids (seq2seq) -> decode\n",
    "    if task == 'text':\n",
    "        if tokenizer is None:\n",
    "            raise ValueError('tokenizer required for text task')\n",
    "        if isinstance(preds, tuple) or hasattr(preds, 'logits'):\n",
    "            preds = preds[0] if isinstance(preds, tuple) else preds.logits\n",
    "        if preds.ndim == 3:\n",
    "            token_ids = np.argmax(preds, axis=-1)\n",
    "        else:\n",
    "            token_ids = preds.astype(int)\n",
    "        decoded = tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "        refs = [ex[text_label_key] for ex in eval_dataset]\n",
    "        metrics = compute_text_metrics(refs, decoded)\n",
    "        return metrics, {'predictions': decoded, 'references': refs}\n",
    "\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    preds = np.asarray(preds)\n",
    "    if preds.ndim > 1 and preds.shape[-1] > 1:\n",
    "        if task == 'classification':\n",
    "            y_pred = np.argmax(preds, axis=-1)\n",
    "        else:\n",
    "            y_pred = preds[:, 0]\n",
    "    else:\n",
    "        y_pred = preds.ravel()\n",
    "\n",
    "    if label_ids is None:\n",
    "        if label_extractor is not None:\n",
    "            y_true = label_extractor(eval_dataset)\n",
    "        else:\n",
    "            try:\n",
    "                y_true = np.array([ex['labels'] for ex in eval_dataset])\n",
    "            except Exception as e:\n",
    "                raise RuntimeError('Could not get labels from trainer output or eval_dataset. Provide label_extractor.') from e\n",
    "    else:\n",
    "        y_true = np.asarray(label_ids).ravel()\n",
    "\n",
    "    if task == 'regression':\n",
    "        metrics = compute_regression_metrics(y_true, y_pred)\n",
    "    elif task == 'classification':\n",
    "        metrics = compute_classification_metrics(y_true, y_pred)\n",
    "    else:\n",
    "        raise ValueError('Unknown task')\n",
    "\n",
    "    return metrics, {'y_true': y_true, 'y_pred': y_pred}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473eabe7",
   "metadata": {},
   "source": [
    "### extract_history_from_trainer(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd261ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_history_from_trainer(trainer):\n",
    "    \"\"\"Return dict with 'train_loss' and any logged eval metrics as lists.\n",
    "    Uses trainer.state.log_history (list of dicts).\n",
    "    \"\"\"\n",
    "    logs = trainer.state.log_history\n",
    "    train_loss = [entry['loss'] for entry in logs if 'loss' in entry]\n",
    "    eval_keys = set(k for e in logs for k in e.keys() if k.startswith('eval_'))\n",
    "    history = {'train_loss': train_loss}\n",
    "    for k in sorted(eval_keys):\n",
    "        history[k] = [e[k] for e in logs if k in e]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a75f8e",
   "metadata": {},
   "source": [
    "### Grad-CAM for CNN models (GradCAM_CNN) and overlay helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c07415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "class GradCAM_CNN:\n",
    "    def __init__(self, model: torch.nn.Module, target_layer: torch.nn.Module):\n",
    "        self.model = model.eval()\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        def forward_hook(module, inp, out):\n",
    "            self.activations = out.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        self.target_layer.register_forward_hook(forward_hook)\n",
    "        self.target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    def __call__(self, input_tensor: torch.Tensor, target_index: int = None) -> np.ndarray:\n",
    "        device = next(self.model.parameters()).device\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        outputs = self.model(input_tensor)\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits = outputs[0]\n",
    "        else:\n",
    "            logits = outputs\n",
    "        if target_index is None:\n",
    "            target_index = int(logits.argmax(dim=-1).cpu().numpy().ravel()[0])\n",
    "        score = logits[:, target_index].squeeze()\n",
    "        self.model.zero_grad()\n",
    "        score.backward(retain_graph=True)\n",
    "        grads = self.gradients.cpu().numpy()[0]\n",
    "        acts = self.activations.cpu().numpy()[0]\n",
    "        weights = np.mean(grads, axis=(1,2))\n",
    "        cam = np.zeros(acts.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam += w * acts[i]\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = cam - cam.min()\n",
    "        if cam.max() > 0:\n",
    "            cam = cam / cam.max()\n",
    "        cam = np.uint8(255 * cam)\n",
    "        cam = Image.fromarray(cam).resize((input_tensor.shape[-1], input_tensor.shape[-2]), resample=Image.BILINEAR)\n",
    "        cam = np.array(cam) / 255.0\n",
    "        return cam\n",
    "\n",
    "def overlay_heatmap(image: np.ndarray, heatmap: np.ndarray, alpha=0.5):\n",
    "    if image.ndim == 2:\n",
    "        image = np.stack([image]*3, axis=-1)\n",
    "    cmap = plt.cm.jet\n",
    "    colored = cmap(heatmap)[:,:,:3]\n",
    "    overlay = (1-alpha)*image + alpha*colored\n",
    "    overlay = np.clip(overlay, 0, 1)\n",
    "    return overlay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bea283",
   "metadata": {},
   "source": [
    "### Grad-CAM for ViT / patch-based models (best-effort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f247ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_cam_vit_patch(model, processor, pil_image, target_token_index=None, model_image_encoder_attr_candidates=None):\n",
    "    \"\"\"Compute a patch-level Grad-CAM heatmap for ViT-style encoders inside HF models.\n",
    "    - model: the HF model (AutoModelForImageTextToText)\n",
    "    - processor: processor to convert pil_image into pixel_values\n",
    "    - pil_image: PIL.Image\n",
    "    - target_token_index: for image->text models this is a proxy; we use last token argmax by default.\n",
    "    - model_image_encoder_attr_candidates: list of attribute paths to try to find the image encoder or patch embedding module in the model.\n",
    "    Returns heatmap resized to original image size (HxW, float [0,1]).\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as F\n",
    "    model.eval()\n",
    "    if model_image_encoder_attr_candidates is None:\n",
    "        model_image_encoder_attr_candidates = [\n",
    "            'vision_model.embeddings.patch_embeddings',\n",
    "            'vision_model.embeddings',\n",
    "            'encoder.vit.embeddings',\n",
    "            'visual_encoder.embeddings',\n",
    "        ]\n",
    "\n",
    "    image_inputs = processor(images=pil_image, return_tensors='pt').to(next(model.parameters()).device)\n",
    "    outputs = model(**image_inputs)\n",
    "    logits = None\n",
    "    if hasattr(outputs, 'logits'):\n",
    "        logits = outputs.logits\n",
    "    elif isinstance(outputs, tuple) and len(outputs) > 0:\n",
    "        logits = outputs[0]\n",
    "    else:\n",
    "        raise RuntimeError('Cannot find logits in model outputs (model-specific).')\n",
    "\n",
    "    if target_token_index is None:\n",
    "        if logits.ndim == 3:\n",
    "            target_token_index = int(logits.argmax(dim=-1)[0, -1].cpu().numpy())\n",
    "        else:\n",
    "            target_token_index = int(logits.argmax(dim=-1)[0].cpu().numpy())\n",
    "\n",
    "    target_module = None\n",
    "    for path in model_image_encoder_attr_candidates:\n",
    "        try:\n",
    "            m = model\n",
    "            for part in path.split('.'):\n",
    "                m = getattr(m, part)\n",
    "            target_module = m\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if target_module is None:\n",
    "        raise RuntimeError('Could not find a suitable image encoder module automatically. Please pass model_image_encoder_attr_candidates pointing to the patch embedding module or final encoder layer.')\n",
    "\n",
    "    activations = None\n",
    "    gradients = None\n",
    "    def forward_hook(module, inp, out):\n",
    "        nonlocal activations\n",
    "        activations = out.detach()\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_out[0].detach()\n",
    "\n",
    "    target_module.register_forward_hook(forward_hook)\n",
    "    target_module.register_backward_hook(backward_hook)\n",
    "\n",
    "    score = logits[0, -1, target_token_index]\n",
    "    model.zero_grad()\n",
    "    score.backward(retain_graph=True)\n",
    "    if activations is None or gradients is None:\n",
    "        raise RuntimeError('Hooks did not capture activations/gradients. You may need to adjust the target module path.')\n",
    "\n",
    "    acts = activations.cpu().numpy()[0]\n",
    "    grads = gradients.cpu().numpy()[0]\n",
    "    if acts.ndim == 2:\n",
    "        acts_t = acts.T\n",
    "        weights = np.mean(grads, axis=0)\n",
    "        cam_patch = np.dot(weights, acts_t)\n",
    "        n_patches = cam_patch.shape[0]\n",
    "        side = int(np.sqrt(n_patches))\n",
    "        cam_map = cam_patch.reshape(side, side)\n",
    "    else:\n",
    "        weights = np.mean(grads, axis=(1,2))\n",
    "        cam_map = np.zeros(acts.shape[1:], dtype=np.float32)\n",
    "        for i, w in enumerate(weights):\n",
    "            cam_map += w * acts[i]\n",
    "    cam_map = np.maximum(cam_map, 0)\n",
    "    if cam_map.max() > 0:\n",
    "        cam_map = cam_map / cam_map.max()\n",
    "    from PIL import Image\n",
    "    cam_img = Image.fromarray(np.uint8(255 * cam_map)).resize(pil_image.size, resample=Image.BILINEAR)\n",
    "    cam_arr = np.array(cam_img) / 255.0\n",
    "    return cam_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578e836",
   "metadata": {},
   "source": [
    "### Example usage (commented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (commented)\n",
    "# history = extract_history_from_trainer(trainer)\n",
    "# plot_metrics(history, title_prefix='Gemma Training')\n",
    "# metrics, raw = evaluate_trainer(trainer, eval_dataset=val_dataset, task='regression')\n",
    "# from PIL import Image\n",
    "# pil = Image.fromarray((val_image*255).astype('uint8')).convert('RGB')\n",
    "# heat = grad_cam_vit_patch(model, processor, pil)\n",
    "# overlay = overlay_heatmap(np.array(pil)/255.0, heat)\n",
    "# plt.imshow(overlay); plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
